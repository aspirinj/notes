---
title: "Regression - Intro"
author: "Howard J"
date: "December 29, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(UsingR)
library(tidyverse)
```



## Intro  
__Regression Analysis__ is a broad term fora set of methodologies used to predict a *Response Variable* (also called a *dependent*,*criterion*, or *outcome* variable) from one or more *Predictor Variables* (also called *independentor*, *explanatory* variables). In general, regression analysis can be used to identifythe explanatory variables that are related to a response variable, to describe the form of the relationships involved, and to provide an equation for predicting the response variable from the explanatory variables.  

*Example:* an exercise physiologist might use regression analysis to develop an equation for predicting the expected number of calories a person will burn while exercising on a treadmill. The response variable is the number of calories burned (calculated from the amount of oxygen consumed), and the predictor variables might include duration of exercise (minutes), percentage of time spent at their target heart rate, average speed (mph), age (years), gender, and body massindex (BMI).

### Tasks
1. **Prediction**, e.g., to use the parent's heights to predict children's heights.  
2. **Modeling**, e.g., to try to find a parsimonious, easily described mean relationship between parental and child heights.
3. **Covariation**, e.g., to investigate the variation in child heights that appears unrelated to parental heights (residual variation) and to quantify what impact genotype information has beyond parental height in explaining child height.

### Galton's Data
This data was created by Francis Galton in 1885. Galton was a statistician who invented the term and concepts of *regression* and *correlation*.  

Let's look at the marginal (parents disregarding children and children disregarding parents) distributions first. The parental distribution is all heterosexual couples. The parental average was corrected for gender via multiplying female heights by 1.08. Remember, Galton didn't have regression to help figure out a better way to do this correction!

```{r}
data(galton)
str(galton)
```

```{r}
galton_long <- gather(galton,key=people,value=height,child:parent)
ggplot(galton_long, aes(x = height, fill = people)) +
    geom_histogram(colour = "black", binwidth=1) + 
    facet_grid(. ~ people)
```

### The Middle
The middle of the outcome Y??

Consider only the children's heights. How could one describe the "middle"?

Let $Y_i$ be the height of child $i$ for $i = 1, \ldots, n = 928$, then define the *middle* as $\mu$ that minimizes:  
$$\sum_{i=1}^n (Y_i - \mu)^2$$

For this sample data, $\mu$ must be the center of mass of the histogram, so that $\mu = \bar Y$. This is called the *least squares estimate* for the **middle** $\mu$.   

Why use **Least Squares**? If there was no variation in the data, every value of $Y_i$ was the same, then there would be no 'error' around the mean. Otherwise, our estimate has to balance the fact that our estimate of $\mu$ isn't going to predict every observation perfectly. Minimizing the **MSE** (Mean Squared Error) seems like one reasonable strategy. We could minimize the average absolute deviation between the data $\mu$ (this leads to the median as the estimate instead of the mean). 


*Derivation:* the reason why the sample average the least squares estimate for $\mu$?

$$
\begin{eqnarray*}
\sum_{i=1}^n (Y_i - \mu)^2 & = & \sum_{i=1}^n (Y_i - \bar Y + \bar Y - \mu)^2 \\
                           & = & \sum_{i=1}^n (Y_i - \bar Y)^2 + 2 \sum_{i=1}^n (Y_i - \bar Y)  (\bar Y - \mu) + \sum_{i=1}^n (\bar Y - \mu)^2 \\
                           & = & \sum_{i=1}^n (Y_i - \bar Y)^2 + 2 (\bar Y - \mu) \sum_{i=1}^n (Y_i - \bar Y)  + \sum_{i=1}^n (\bar Y - \mu)^2 \\
                           & = & \sum_{i=1}^n (Y_i - \bar Y)^2 + 2 (\bar Y - \mu)  (\sum_{i=1}^n Y_i - n \bar Y) + \sum_{i=1}^n (\bar Y - \mu)^2 \\
                           & = & \sum_{i=1}^n (Y_i - \bar Y)^2 + \sum_{i=1}^n (\bar Y - \mu)^2\\
                           & \geq & \sum_{i=1}^n (Y_i - \bar Y)^2 \
\end{eqnarray*}
$$


**Tweaking the MSE:**  
```{r}
mu <- 65
mse <- mean((galton$child - mu)^2) # calculating the mean squared error
ggplot(galton, aes(x = child)) + 
    geom_histogram(fill = "salmon", colour = "black", binwidth=1) + 
    geom_vline(xintercept = mu, size = 2) + 
    ggtitle(paste("mu = ", mu, ", MSE = ", round(mse, 2), sep = ""))
```

**Minimize MSE:**  The least squares estimate is the empirical mean.  
```{r}
mu <- mean(galton$child)
mse <- mean((galton$child - mu)^2) # calculating the mean squared error
ggplot(galton, aes(x = child)) + 
    geom_histogram(fill = "salmon", colour = "black", binwidth=1) + 
    geom_vline(xintercept = mu, size = 2) + 
    ggtitle(paste("mu = ", mu, ", MSE = ", round(mse, 2), sep = ""))
```


Size of point represents number of points at that (X, Y) combination (See the Rmd file for the code).

```{r freqGalton, fig.height=6, fig.width=6}
freqData <- as.data.frame(table(galton$child, galton$parent)) # Make the contingency table a long data frame
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")                    
g
```

