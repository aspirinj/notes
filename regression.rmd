---
title: "Regression - Intro"
author: "Howard J"
date: "December 29, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(UsingR)
library(tidyverse)
```



## Intro  
__Regression Analysis__ is a broad term fora set of methodologies used to predict a *Response Variable* (also called a *dependent*,*criterion*, or *outcome* variable) from one or more *Predictor Variables* (also called *independentor*, *explanatory* variables). In general, regression analysis can be used to identifythe explanatory variables that are related to a response variable, to describe the form of the relationships involved, and to provide an equation for predicting the response variable from the explanatory variables.  

*Example:* an exercise physiologist might use regression analysis to develop an equation for predicting the expected number of calories a person will burn while exercising on a treadmill. The response variable is the number of calories burned (calculated from the amount of oxygen consumed), and the predictor variables might include duration of exercise (minutes), percentage of time spent at their target heart rate, average speed (mph), age (years), gender, and body massindex (BMI).

### Tasks
1. **Prediction**, e.g., to use the parent's heights to predict children's heights.  
2. **Modeling**, e.g., to try to find a parsimonious, easily described mean relationship between parental and child heights.
3. **Covariation**, e.g., to investigate the variation in child heights that appears unrelated to parental heights (residual variation) and to quantify what impact genotype information has beyond parental height in explaining child height.

### Galton's Data
This data was created by Francis Galton in 1885. Galton was a statistician who invented the term and concepts of *regression* and *correlation*.  

Let's look at the marginal (parents disregarding children and children disregarding parents) distributions first. The parental distribution is all heterosexual couples. The parental average was corrected for gender via multiplying female heights by 1.08. Remember, Galton didn't have regression to help figure out a better way to do this correction!

```{r galton}
data(galton)
str(galton)
```

```{r}
galton_long <- gather(galton,key=people,value=height,child:parent)
ggplot(galton_long, aes(x = height, fill = people)) +
    geom_histogram(colour = "black", binwidth=1) + 
    facet_grid(. ~ people)
```

### The Middle
The middle of the outcome Y??

Consider only the children's heights. How could one describe the "middle"?

Let $Y_i$ be the height of child $i$ for $i = 1, \ldots, n = 928$, then define the *middle* as $\mu$ that minimizes:  
$$\sum_{i=1}^n (Y_i - \mu)^2$$

For this sample data, $\mu$ must be the center of mass of the histogram, so that $\mu = \bar Y$. This is called the *least squares estimate* for the **middle** $\mu$.   

Why use **Least Squares**? If there was no variation in the data, every value of $Y_i$ was the same, then there would be no 'error' around the mean. Otherwise, our estimate has to balance the fact that our estimate of $\mu$ isn't going to predict every observation perfectly. Minimizing the **MSE** (Mean Squared Error) seems like one reasonable strategy. We could minimize the average absolute deviation between the data $\mu$ (this leads to the median as the estimate instead of the mean). 


*Derivation:* the reason why the sample average the least squares estimate for $\mu$?

$$
\begin{eqnarray*}
\sum_{i=1}^n (Y_i - \mu)^2 & = & \sum_{i=1}^n (Y_i - \bar Y + \bar Y - \mu)^2 \\
                           & = & \sum_{i=1}^n (Y_i - \bar Y)^2 + 2 \sum_{i=1}^n (Y_i - \bar Y)  (\bar Y - \mu) + \sum_{i=1}^n (\bar Y - \mu)^2 \\
                           & = & \sum_{i=1}^n (Y_i - \bar Y)^2 + 2 (\bar Y - \mu) \sum_{i=1}^n (Y_i - \bar Y)  + \sum_{i=1}^n (\bar Y - \mu)^2 \\
                           & = & \sum_{i=1}^n (Y_i - \bar Y)^2 + 2 (\bar Y - \mu)  (\sum_{i=1}^n Y_i - n \bar Y) + \sum_{i=1}^n (\bar Y - \mu)^2 \\
                           & = & \sum_{i=1}^n (Y_i - \bar Y)^2 + \sum_{i=1}^n (\bar Y - \mu)^2\\
                           & \geq & \sum_{i=1}^n (Y_i - \bar Y)^2 \
\end{eqnarray*}
$$


**Tweaking the MSE:**  
```{r}
mu <- 65
mse <- mean((galton$child - mu)^2) # calculating the mean squared error
ggplot(galton, aes(x = child)) + 
    geom_histogram(fill = "salmon", colour = "black", binwidth=1) + 
    geom_vline(xintercept = mu, size = 2) + 
    ggtitle(paste("mu = ", mu, ", MSE = ", round(mse, 2), sep = ""))
```

**Minimize MSE:**  The least squares estimate is the empirical mean.  
```{r}
mu <- mean(galton$child)
mse <- mean((galton$child - mu)^2) # calculating the mean squared error
ggplot(galton, aes(x = child)) + 
    geom_histogram(fill = "salmon", colour = "black", binwidth=1) + 
    geom_vline(xintercept = mu, size = 2) + 
    ggtitle(paste("mu = ", mu, ", MSE = ", round(mse, 2), sep = ""))
```


### Plot Galton data
Notice that the overplotting is hiding a lot of info.  
```{r, dependson="galton",fig.height=4,fig.width=4, fig.align='center'}
ggplot(galton, aes(x = parent, y = child)) + geom_point()
```

Use the size of points to represent the count of points at that (X, Y) combination.  
```{r Galton, fig.height=6, fig.width=6}
freqData <- as.data.frame(table(galton$child, galton$parent)) # Make the contingency table a long data frame
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
ggplot(filter(freqData, freq > 0), aes(x = parent, y = child)) + 
  scale_size(range = c(2, 20), guide = "none" ) + 
  geom_point(colour="grey50", aes(size = freq+5)) + 
  geom_point(aes(colour=freq, size = freq)) + 
  scale_colour_gradient(low = "lightblue", high="white")
```


### Regression through the origin
$X_i$ are the parents' heights; $Y_i$ are the children's heights; $\beta$ is the slope that minimizes $\sum_{i=1}^n (Y_i - X_i \beta)^2$.  
Shift and center the data to the mean. Assume $\beta=1$ and calculate MSE.  

```{r}
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))

beta = 1
mse <- mean( (y - beta * x) ^2 )

ggplot(filter(freqData, freq > 0), aes(x = parent, y = child)) + 
  scale_size(range = c(2, 20), guide = "none" ) + 
  geom_point(colour="grey50", aes(size = freq+5)) + 
  geom_point(aes(colour=freq, size = freq)) + 
  scale_colour_gradient(low = "lightblue", high="white") + 
  geom_abline(intercept = 0, slope = beta, size = 1) + 
  ggtitle(paste("beta = ", beta, "mse = ", round(mse, 3)))
```


Calculation of the centered data:  
```{r}
lm(I(child - mean(child))~ I(parent - mean(parent)) - 1, data = galton)
```

---
```{r, fig.height=6,fig.width=7}
freqData <- as.data.frame(table(galton$child, galton$parent)) # Make the contingency table a long data frame
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))

lm1 <- lm(galton$child ~ galton$parent)
beta = coef(lm1)[2]
mse <- mean( (y - beta * x) ^2 )

ggplot(filter(freqData, freq > 0), aes(x = parent, y = child)) + 
  scale_size(range = c(2, 20), guide = "none" ) + 
  geom_point(colour="grey50", aes(size = freq+5)) + 
  geom_point(aes(colour=freq, size = freq)) + 
  scale_colour_gradient(low = "lightblue", high="white") + 
  geom_abline(intercept = coef(lm1)[1], slope = coef(lm1)[2], size = 1) + 
  ggtitle(paste("beta = ", round(beta,3), "mse = ", round(mse, 3)))

```


## Notation
### Data
*Alphabetical letters:*  
$X_1, X_2, \ldots, X_n$, $Y_1, \ldots , Y_n$: *n* data points.  
*Example:* the data set $\{1, 2, 5\}$: $X_1 = 1$, $X_2 = 2$, $X_3 = 5$ and $n = 3$.  

*Greek letters:*  
Use Greek letters for things we don't know.  
*Example:* $\mu$ is a mean to estimate.  

### Emperical values
Emperical values = Sample values

#### The empirical mean 
$$\bar X = \frac{1}{n}\sum_{i=1}^n X_i$$  

*Notice:* "centering" the random variables:   
$$\tilde X_i = X_i - \bar X$$
Where the mean of the $\tilde X_i$ is 0.  

The sample average $\bar X$ is the least squares estimate for $\mu$. For the least squares solution for minimizing:  
$$\sum_{i=1}^n (X_i - \mu)^2$$  
It can be centered and rewritten as:  
$$\sum_{i=1}^n (\tilde X_i)^2$$  


#### The emprical standard deviation and variance
Empirical variance:  
$$S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar X)^2 = \frac{1}{n-1} \left( \sum_{i=1}^n X_i^2 - n \bar X ^ 2 \right)$$  

* The empirical standard deviation is defined as
$S = \sqrt{S^2}$. Notice that the standard deviation has the same units as the data.
* The data defined by $X_i / s$ have empirical standard deviation 1. This is called "scaling" the data.

---
## Normalization

* The data defined by
$$
Z_i = \frac{X_i - \bar X}{s}
$$
have empirical mean zero and empirical standard deviation 1. 
* The process of centering then scaling the data is called "normalizing" the data. 
* Normalized data are centered at 0 and have units equal to standard deviations of the original data. 
* Example, a value of 2 from normalized data means that data point
was two standard deviations larger than the mean.
