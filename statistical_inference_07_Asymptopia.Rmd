---
title: "Statistical Inference Notes"
author: "Howard J"
date: "December 14, 2017"
output:
html_document:
  highlight: pygments
  theme: spacelab
  toc: yes
header-includes: \usepackage{graphicx}
---

```{r message=FALSE, warning=FALSE}
library(UsingR)
library(tidyverse)
library(grid)
library(png)
```


## Asymptotics  
Asymptotics is the term for the behavior of statistics as the sample size limits to infinity.    
Asymptotics are incredibly useful for simple statistical inference and approximations. Asymptotics also often lead to nice understanding of procedures.   
Asymptotics generally give no assurances about finite sample performance. Asymptotics form the basis for frequency interpretation of probabilities (the long run proportion of times an event occurs).  


### Limits of random variables
Fortunately, for the sample mean there's a set of powerful results. These results allow us to talk about the large sample distribution of sample means of a collection of $iid$ observations.  
Law of Large Numbers says that the average limits to what its estimating, the population mean. The LLN forms the basis of frequency style thinking.  
*Example:* $\bar X_n$ could be the average of the result of $n$ coin flips (i.e. the sample proportion of heads). As we flip a fair coin over and over, it evetually converges to the true probability of a head. 
An good estimator is __Consistent__ if it converges to what you want to estimate. The LLN says that the sample mean of iid sample is consistent for the population mean.   
The sample variance and the sample standard deviation of iid random variables are consistent as well.  



__LLN, sample average of standard normal distribution__  
```{r, fig.height=5, fig.width=5}
# n - total count of trials, or sample size
# x_val - number of trials
# y_val - cumulative means
n <- 10000
df <- data_frame(x_val = 1:n, y_val = cumsum(rnorm(n))/(1:n))
ggplot(df, aes(x=x_val, y=y_val)) + 
    geom_hline(yintercept=0) + 
    geom_line(size=2) + 
    labs(x="Number of obs", y="Cumulative mean")
```


__LLN, sample average of coin flips__  
```{r, fig.height=5, fig.width=5}

# x_val - number of trials
# y_val - cumulative means
n <- 10000
df <- data.frame(x_val=1:n, y_val=cumsum(sample(0:1,n,replace=TRUE))/(1:n))
ggplot(df, aes(x=x_val, y=y_val)) + 
    geom_hline(yintercept=0.5) + geom_line(size=2) + 
    labs(x="Number of obs", y="Cumulative mean")
```



### The Central Limit Theorem  
The CLT states that the distribution of averages of iid variables (properly normalized) becomes that of a standard normal as the sample size increases. 
$$\frac{\bar X_n - \mu}{\sigma / \sqrt{n}}=
\frac{\sqrt n (\bar X_n - \mu)}{\sigma}
= \frac{\mbox{Estimate} - \mbox{Mean of estimate}}{\mbox{Std. Err. of estimate}}$$  
has a distribution like that of a standard normal for large $n$.
* (Replacing the standard error by its estimated value doesn't change the CLT)
* The useful way to think about the CLT is that $\bar X_n$ is approximately $N(\mu, \sigma^2 / n)$



*Example:* simulate a standard normal random variable by rolling $n$ (six sided). $X_i$ be the outcome for die $i$. Thus,  
$\mu=E[X_i]=3.5$, $Var(X_i)=2.92$  
SE $\sqrt{2.92 / n}=1.71 / \sqrt{n}$
- Lets roll $n$ dice, take their mean, subtract off 3.5,
and divide by $1.71 / \sqrt{n}$ and repeat this over and over


## Result of our die rolling experiment  
```{r, fig.width=9, fig.height=6, fig.align='center'}
# n - total count of trials, or sample size
sim_count <- 1000
cfunc <- function(x, n) sqrt(n)*(mean(x) - 3.5) / 1.71
df <- data.frame(
  x_val=c(apply(matrix(sample(1:6, sim_count*10, replace=TRUE), sim_count), 1, cfunc, 10),
        apply(matrix(sample(1:6, sim_count*20, replace=TRUE), sim_count), 1, cfunc, 20),
        apply(matrix(sample(1:6, sim_count*30, replace=TRUE), sim_count), 1, cfunc, 30)
        ),
  sample_size=factor(rep(c(10, 20, 30), rep(sim_count, 3))))
ggplot(df, aes(x=x_val, fill=sample_size)) + 
    geom_histogram(alpha=.20, binwidth=.3, colour="black", aes(y=..density..)) + 
    stat_function(fun=dnorm, size=2) + 
    facet_grid(. ~ sample_size)
```


## Coin CLT  
Let $X_i$ be the $0$ or $1$ result of the $i^{th}$ flip of a possibly unfair coin. The sample proportion, say $\hat p$, is the average of the coin flips.  
$$E[X_i] = p$$  
$$Var(X_i) = p(1-p)$$  
Thus, $\sigma = \sqrt{p(1-p)/n}$  


$$\frac{\hat p - p}{\sqrt{p(1-p)/n}}$$    
will be approximately normally distributed. Let's flip a coin $n$ times, take the sample proportion of heads, subtract off .5 and multiply the result by $2 \sqrt{n}$ (divide by $1/(2 \sqrt{n})$)


## Simulation results
```{r, fig.width=9, fig.height=6, fig.align='center'}
sim_count <- 1000
cfunc <- function(x, n) 2*sqrt(n)*(mean(x) - 0.5) 
df <- data.frame(
  x=c(apply(matrix(sample(0:1, sim_count*10, replace=TRUE), sim_count), 1, cfunc, 10),
        apply(matrix(sample(0:1, sim_count*20, replace=TRUE), sim_count), 1, cfunc, 20),
        apply(matrix(sample(0:1, sim_count*30, replace=TRUE), sim_count), 1, cfunc, 30)
        ),
  size=factor(rep(c(10, 20, 30), rep(sim_count, 3))))

ggplot(df, aes(x=x, fill=size)) + geom_histogram(binwidth=.3, colour="black", aes(y=..density..)) + 
    stat_function(fun=dnorm, size=2) +
    facet_grid(. ~ size)
```

---
## Simulation results, $p=0.9$
```{r, fig.width=9, fig.height=6, fig.align='center'}
sim_count <- 1000
cfunc <- function(x, n) sqrt(n)*(mean(x) - 0.9) / sqrt(.1*.9)
df <- data.frame(
  x=c(apply(matrix(sample(0:1, prob=c(.1,.9), sim_count*10, replace=TRUE), sim_count), 1, cfunc, 10),
        apply(matrix(sample(0:1, prob=c(.1,.9), sim_count*20, replace=TRUE), sim_count), 1, cfunc, 20),
        apply(matrix(sample(0:1, prob=c(.1,.9), sim_count*30, replace=TRUE), sim_count), 1, cfunc, 30)
        ),
  size=factor(rep(c(10, 20, 30), rep(sim_count, 3))))
ggplot(df, aes(x=x, fill=size)) + 
    geom_histogram(binwidth=.3, colour="black", aes(y=..density..)) + 
    stat_function(fun=dnorm, size=2) +
    facet_grid(. ~ size)
```

---
### Galton's quincunx 


### Confidence intervals  
According to the CLT, the sample mean, $\bar X$, is approximately normal with mean $\mu$ and standard deviation $\sigma / \sqrt{n}$.  
$\mu + 2 \sigma /\sqrt{n}$ is pretty far out in the right tail (only 2.5% of a normal being larger than 2 standard deviations in the tail)  
So the probability $\bar X$ is bigger than $\mu + 2 \sigma / \sqrt{n}$ or smaller than $\mu - 2 \sigma / \sqrt{n}$ is 5%.  

For average of random sample from a population $\bar X$:  
The quantity $\bar X \pm 2 \sigma /\sqrt{n}$ is called a 95% interval for $\mu$.
The factor '2' is rounded from '1.96' corresponds to the 97.5th quantile. It corresponds to the 95% interval, refering to the fact that if one were to repeatly get samples of size $n$, about 95% of the intervals obtained would contain $\mu$.  
90% interval corresponds to 95th percentile (1.645).


## Give a confidence interval for the average height of sons
in Galton's data
```{r}
library(UsingR);data(father.son); x <- father.son$sheight
(mean(x) + c(-1, 1)*qnorm(.975)*sd(x) / sqrt(length(x))) / 12
```
*Note:* calculation is divided by 12 to get the interval in feet.

---

## Sample proportions

- In the event that each $X_i$ is $0$ or $1$ with common success probability $p$ then $\sigma^2=p(1 - p)$
- The interval takes the form
$$
    \hat p \pm z_{1 - \alpha/2}  \sqrt{\frac{p(1 - p)}{n}}
$$
- Replacing $p$ by $\hat p$ in the standard error results in what is called a Wald confidence interval for $p$
- For 95% intervals
$$\hat p \pm \frac{1}{\sqrt{n}}$$ 
is a quick CI estimate for $p$

---
## Example
* Your campaign advisor told you that in a random sample of 100 likely voters 56 intent to vote for you. 
 *Can you relax? Do you have this race in the bag?
 *Without access to a computer or calculator, how precise is this estimate?
* `1/sqrt(100)=0.1` so a back of the envelope calculation gives an approximate 95% interval of `(0.46, 0.66)`
 *Not enough for you to relax, better go do more campaigning!
* Rough guidelines, 100 for 1 decimal place, 10,000 for 2, 1,000,000 for 3.
```{r}
round(1 / sqrt(10 ^ (1:6)), 3)
```



---
## Binomial interval

```{r}
.56 + c(-1, 1)*qnorm(.975)*sqrt(.56*.44 / 100)
binom.test(56, 100)$conf.int
```

---

## Simulation

```{r}
n <- 20; pvals <- seq(.1, .9, by=.05); sim_count <- 1000
coverage <- sapply(pvals, function(p){
  phats <- rbinom(sim_count, prob=p, size=n) / n
  ll <- phats - qnorm(.975)*sqrt(phats*(1 - phats) / n)
  ul <- phats + qnorm(.975)*sqrt(phats*(1 - phats) / n)
  mean(ll < p & ul > p)
})

```


---
## Plot of the results (not so good)
```{r, fig.align='center', fig.height=6, fig.width=6}
ggplot(data.frame(pvals, coverage), aes(x=pvals, y=coverage)) + geom_line(size=2) + geom_hline(yintercept=0.95) + ylim(.75, 1.0)
```

---
## What's happening?
- $n$ isn't large enough for the CLT to be applicable
for many of the values of $p$
- Quick fix, form the interval with 
$$
\frac{X + 2}{n + 4}
$$
- (Add two successes and failures, Agresti/Coull interval)

---
## Simulation
First let's show that coverage gets better with $n$

```{r}
n <- 100; pvals <- seq(.1, .9, by=.05); sim_count <- 1000
coverage2 <- sapply(pvals, function(p){
  phats <- rbinom(sim_count, prob=p, size=n) / n
  ll <- phats - qnorm(.975)*sqrt(phats*(1 - phats) / n)
  ul <- phats + qnorm(.975)*sqrt(phats*(1 - phats) / n)
  mean(ll < p & ul > p)
})

```

---
## Plot of coverage for $n=100$
```{r, fig.align='center', fig.height=6, fig.width=6}
ggplot(data.frame(pvals, coverage), aes(x=pvals, y=coverage2)) + geom_line(size=2) + geom_hline(yintercept=0.95)+ ylim(.75, 1.0)
```

---
## Simulation
Now let's look at $n=20$ but adding 2 successes and failures
```{r}
n <- 20; pvals <- seq(.1, .9, by=.05); sim_count <- 1000
coverage <- sapply(pvals, function(p){
  phats <- (rbinom(sim_count, prob=p, size=n) + 2) / (n + 4)
  ll <- phats - qnorm(.975)*sqrt(phats*(1 - phats) / n)
  ul <- phats + qnorm(.975)*sqrt(phats*(1 - phats) / n)
  mean(ll < p & ul > p)
})
```


---
## Adding 2 successes and 2 failures
(It's a little conservative)
```{r, fig.align='center', fig.height=6, fig.width=6}
ggplot(data.frame(pvals, coverage), aes(x=pvals, y=coverage)) + geom_line(size=2) + geom_hline(yintercept=0.95)+ ylim(.75, 1.0)
```

---

## Poisson interval
* A nuclear pump failed 5 times out of 94.32 days, give a 95% confidence interval for the failure rate per day?
* $X \sim Poisson(\lambda t)$.
* Estimate $\hat \lambda=X/t$
* $Var(\hat \lambda)=\lambda / t$ 
* $\hat \lambda / t$ is our variance estimate

---
## R code
```{r}
x <- 5; t <- 94.32; lambda <- x / t
round(lambda + c(-1, 1)*qnorm(.975)*sqrt(lambda / t), 3)
poisson.test(x, T=94.32)$conf
```


---
## Simulating the Poisson coverage rate
Let's see how this interval performs for lambda
values near what we're estimating
```{r}
lambdavals <- seq(0.005, 0.10, by=.01); sim_count <- 1000
t <- 100
coverage <- sapply(lambdavals, function(lambda){
  lhats <- rpois(sim_count, lambda=lambda*t) / t
  ll <- lhats - qnorm(.975)*sqrt(lhats / t)
  ul <- lhats + qnorm(.975)*sqrt(lhats / t)
  mean(ll < lambda & ul > lambda)
})
```



---
## Covarage
(Gets really bad for small values of lambda)
```{r, fig.align='center', fig.height=6, fig.width=6}
ggplot(data.frame(lambdavals, coverage), aes(x=lambdavals, y=coverage)) + geom_line(size=2) + geom_hline(yintercept=0.95)+ylim(0, 1.0)
```


## What if we increase t to 1000?
```{r, fig.align='center', fig.height=6, fig.width=6}
lambdavals <- seq(0.005, 0.10, by=.01)
sim_count <- 1000
t <- 1000
coverage <- sapply(lambdavals, function(lambda){
  lhats <- rpois(sim_count, lambda=lambda*t) / t
  ll <- lhats - qnorm(.975)*sqrt(lhats / t)
  ul <- lhats + qnorm(.975)*sqrt(lhats / t)
  mean(ll < lambda & ul > lambda)
})
ggplot(data.frame(lambdavals, coverage), aes(x=lambdavals, y=coverage)) + geom_line(size=2) + geom_hline(yintercept=0.95) + ylim(0, 1.0)
```

The __LLN__ states that averages of iid samples converge to the population means that they are estimating.  
The __CLT__ states that averages are approximately normal, with distributions.  
* Centered at the population mean  
* with standard deviation equal to the standard error of the mean  
* CLT gives no guarantee that $n$ is large enough  
Taking the mean and adding and subtracting the relevant normal quantile times the SE yields a __Confidence Interval__ for the mean. Adding and subtracting 2 SEs works for 95% intervals.   
* Confidence intervals get narrower with less variability or larger sample sizes.  
* The Poisson and binomial case have exact intervals that don't require the CLT. But a quick fix for small sample size binomial calculations is to add 2 successes and failures
