# Week 2  
## Variability  
Variability has many important uses in statistics. First, the population variance is itself an intrinsically interesting quantity that we want to estimate. Secondly, variability in our estimates is what makes them not imprecise.

```{r fig.width = 4, fig.height = 3, fig.align = 'center', message = F, warning = F}
# generate x value ranges
xvals <- seq(-10, 10, by = .01)
# generate data from normal distribution for sd of 1 to 4
dat <- data.frame(
    y = c(dnorm(xvals, mean = 0, sd = 1),
        dnorm(xvals, mean = 0, sd = 2),
        dnorm(xvals, mean = 0, sd = 3),
        dnorm(xvals, mean = 0, sd = 4)),
    x = rep(xvals, 4),
    factor = factor(rep(1 : 4, rep(length(xvals), 4)))
)
# plot 4 lines for the different standard deviations
ggplot(dat, aes(x = x, y = y, color = factor)) + geom_line(size = 2)
```


* __Variance__ is the measure of spread or dispersion, the expected squared distance of the variable from its mean. As we can see from above, (higher variances $\rightarrow$ more spread), (lower variances $\rightarrow$ smaller spread).  
$$Var(X) = E[(X-\mu)^2] = E[X^2] - E[X]^2$$  
* __Standard Deviation__, the reason for taking the standard deviation is because that measure has the same units as the population.  
$$\sigma = \sqrt{Var(X)}$$  
* *Example:* for random die roll, $E[X] = 3.5$  
		$E[X^2] = 1^2 \times 1/6 + 2^2 \times 1/6 + 3^2 \times 1/6 + 4^2 \times 1/6 + 5^2 \times 1/6 + 6^2 \times 1/6 = 15.17$  
		$Var(X) = E[X^2] - E[X]^2 \approx 2.92$  
* *Example:* for coin flip, $E[X] = p$  
		$E[X^2] = 0^2 \times (1 - p) + 1^2 \times p= p$  
		$Var(X) = E[X^2] - E[X]^2 = p - p^2 = p(1-p)$

### Sample Variance  
* __Sample Variance__ is the estimator of the population variance. defined as:  
$$S^2 = \frac{\sum_{i=1} (X_i - \bar X)^2}{n-1}$$  

```{r echo = FALSE, fig.width = 4, fig.height = 3, fig.align = 'center'}
grid.raster(readPNG("figures/6.png"))
```
The __Sample Variance__ will most likely have a variance that is *lower than* the __Population Variance__.  
* If a sample is as long as the population, the calculation should times $1 \over n$, for $n$ trials. But the fact is that any sample will most likely have a lower variance than the population variance.  
* Thus, the correct calculation is to times $1 \over (n - 1)$ will make the variance estimator *larger* to adjust $\rightarrow$ leads to more conservative estimation $\rightarrow$ $S^2$ is so called ***unbiased estimate of population variance***
    * $S^2$ is a random variable, and therefore has an associated population distribution
		* $E[S^2]$ = population variance, where $S$ = sample standard deviation
		* as we see from the simulation results below, with more data, the distribution for $S^2$ gets more concentrated around population variance

```{r fig.width = 4, fig.height = 3, fig.align = 'center'}
# specify number of simulations
nosim <- 10000;
# simulate data for various sample sizes
dat <- data.frame(
    x = c(apply(matrix(rnorm(nosim * 10), nosim), 1, var),
          apply(matrix(rnorm(nosim * 20), nosim), 1, var),
          apply(matrix(rnorm(nosim * 30), nosim), 1, var)),
    n = factor(rep(c("10", "20", "30"), c(nosim, nosim, nosim))) )
# plot density function for different sample size data
ggplot(dat, aes(x = x, fill = n)) + geom_density(size = 1, alpha = .2) +
	geom_vline(xintercept = 1, size = 1)
```

* ***Note**: for any variable, properties of the population = **parameter**, estimates of properties for samples = **statistic** *
	- below is a summary for the mean and variance for population and sample

```{r echo = FALSE, fig.width = 6, fig.height = 5, fig.align = 'center'}
grid.raster(readPNG("figures/8.png"))
```

* **distribution for mean of random samples**
	* expected value of the **mean** of distribution of means = expected value of the sample mean = population mean
		* $E[\bar X]=\mu$
	* expected value of the variance of distribution of means
		* $Var(\bar X) = \sigma^2/n$
		* as **n** becomes larger, the mean of random sample $\rightarrow$ more concentrated around the population mean $\rightarrow$ variance approaches 0
			* this again confirms that sample mean estimates population mean
	* ***Note**: normally we only have 1 sample mean (from collected sample) and can estimate the variance $\sigma^2$ $\rightarrow$ so we know a lot about the **distribution of the means** from the data observed *

* **standard error (SE)**
	* the standard error of the mean is defined as $$SE_{mean} = \sigma/\sqrt{n}$$
	* this quantity is effectively the standard deviation of the distribution of a statistic (i.e. mean)
	* represents variability of means


### Entire Estimator-Estimation Relationship
* Start with a sample
* $S^2$ = sample variance
	* estimates how variable the population is
	* estimates population variance $\sigma^2$
	* $S^2$ = a random variable and has its own distribution centered around $\sigma^2$
		* more concentrated around $\sigma^2$ as $n$ increases
* $\bar X$ = sample mean
	* estimates population mean $\mu$
	* $\bar X$ = a random variable and has its own distribution centered around $\mu$
		* more concentrated around $\mu$ as $n$ increases
		* variance of distribution of $\bar X = \sigma^2/n$
		* estimate of variance = $S^2/n$
		* estimate of standard error = $S/\sqrt{n}$ $\rightarrow$ "sample standard error of the mean"
			- estimates how variable sample means ($n$ size) from the population are

### Example - Standard Normal
* variance = 1
* means of **n** standard normals (sample) have standard deviation = $1/\sqrt{n}$

```{r message = F, warning = F}
# specify number of simulations with 10 as number of observations per sample
nosim <- 1000; n <-10
# estimated standard deviation of mean
sd(apply(matrix(rnorm(nosim * n), nosim), 1, mean))
# actual standard deviation of mean of standard normals
1 / sqrt(n)
```

* `rnorm()` = generate samples from the standard normal
* `matrix()` = puts all samples into a nosim by $n$ matrix, so that each row represents a simulation with `nosim` observations
* `apply()` = calculates the mean of the $n$ samples
* `sd()` = returns standard deviation

### Example - Standard Uniform
* standard uniform $\rightarrow$ triangle straight line distribution $\rightarrow$ mean = 1/2 and variance = 1/12
* means of random samples of $n$ uniforms have have standard deviation of $1/\sqrt{12 \times n}$


```{r message = F, warning = F}
# estimated standard deviation of the sample means
sd(apply(matrix(runif(nosim * n), nosim), 1, mean))
# actual standard deviation of the means
1/sqrt(12*n)
```


### Example - Poisson
* $Poisson(x^2)$ have variance of $x^2$
* means of random samples of $n~ Poisson(4)$ have standard deviation of $2/\sqrt{n}$

```{r message = F, warning = F}
# estimated standard deviation of the sample means
sd(apply(matrix(rpois(nosim * n, lambda=4), nosim), 1, mean))
# actual standard deviation of the means
2/sqrt(n)
```


### Example - Bernoulli
* for $p = 0.5$, the Bernoulli distribution has variance of 0.25
* means of random samples of $n$ coin flips have standard deviations of $1 / (2 \sqrt{n})$

```{r message = F, warning = F}
# estimated standard deviation of the sample means
sd(apply(matrix(sample(0 : 1, nosim * n, replace = TRUE), nosim), 1, mean))
# actual standard deviation of the means
1/(2*sqrt(n))
```

### Example - Father/Son

```{r fig.width = 4, fig.height = 3, fig.align = 'center', message = F, warning = F}
# load data
library(UsingR); data(father.son);
# define son height as the x variable
x <- father.son$sheight
# n is the length
n<-length(x)
# plot histogram for son's heights
g <- ggplot(data = father.son, aes(x = sheight))
g <- g + geom_histogram(aes(y = ..density..), fill = "lightblue", binwidth=1, colour = "black")
g <- g + geom_density(size = 2, colour = "black")
g
# we calculate the parameters for variance of distribution and sample mean,
round(c(sampleVar = var(x),
	sampleMeanVar = var(x) / n,
	# as well as standard deviation of distribution and sample mean
	sampleSd = sd(x),
	sampleMeanSd = sd(x) / sqrt(n)),2)
```





## Binomial Distribution
* **binomial random variable** = sum of **n** Bernoulli variables $$X = \sum_{i=1}^n X_i$$ where $X_1,\ldots,X_n = Bernoulli(p)$
	* PMF is defined as $$P(X=x) = {n \choose x}p^x(1-p)^{n-x}$$ where ${n \choose x}$ = number of ways selecting $x$ items out of $n$ options without replacement or regard to order and for $x=0,\ldots,n$
	* **combination** or "$n$ choose $x$" is defined as $${n \choose x} = \frac{n!}{x!(n-x)!}$$
	* the base cases are $${n \choose n} = {n \choose 0} = 1$$
* **Bernoulli distribution** = binary outcome
	* only possible outcomes
		* 1 = "success" with probability of $p$
		* 0 = "failure" with probability of $1 - p$
	* PMF is defined as $$P(X=x) = p^x(1 - p)^{1-x}$$
	* mean = $p$
	* variance = $p(1 - p)$

### Example
* of 8 children, whats the probability of 7 or more girls (50/50 chance)?
$${8 \choose 7}.5^7(1-.5)^{1} + {8 \choose 8}.5^8(1-.5)^{0} \approx 0.04$$

```{r message = F, warning = F}
# calculate probability using PMF
choose(8, 7) * .5 ^ 8 + choose(8, 8) * .5 ^ 8
# calculate probability using CMF from distribution
pbinom(6, size = 8, prob = .5, lower.tail = FALSE)
```

* `choose(8, 7)` = R function to calculate n choose x
* `pbinom(6, size=8, prob =0.5, lower.tail=TRUE)` = probability of 6 or less successes out of 8 samples with probability of 0.5 (CMF)
	- `lower.tail=FALSE` = returns the complement, in this case it's the probability of greater than 6 successes out of 8 samples with probability of 0.5




## Normal Distribution
* normal/Gaussian distribution for random variable X
	* notation = $X \sim N(\mu, \sigma^2)$
	* mean = $E[X] = \mu$
	* variance = $Var(X) = \sigma^2$
	* PMF is defined as $$f(x)=(2\pi\sigma^2)^{-1/2}e^{-(x-\mu)^2/2\sigma^2}$$
* $X \sim N(0, 1)$ = **standard normal distribution** (standard normal random variables often denoted using $Z_1, Z_2, \ldots$)
	- ***Note**: see below graph for reference for the following observations *
	* ~68% of data/normal density $\rightarrow$ between $\pm$ 1 standard deviation from $\mu$
	* ~95% of data/normal density $\rightarrow$ between $\pm$ 2 standard deviation from $\mu$
	* ~99% of data/normal density $\rightarrow$ between $\pm$ 3 standard deviation from $\mu$
	* $\pm$ 1.28 standard deviations from $\mu$ $\rightarrow$ 10$^{th}$ (-) and 90$^{th}$ (+) percentiles
	* $\pm$ 1.645 standard deviations from $\mu$ $\rightarrow$ 5$^{th}$ (-) and 95$^{th}$ (+) percentiles
	* $\pm$ 1.96 standard deviations from $\mu$ $\rightarrow$ 2.5$^{th}$ (-) and 97.5$^{th}$ (+) percentiles
	* $\pm$ 2.33 standard deviations from $\mu$ $\rightarrow$ 1$^{st}$ (-) and 99$^{th}$ (+) percentiles

```{r fig.width = 4, fig.height = 3, fig.align = 'center', message = F, warning = F}
# plot standard normal
x <- seq(-3, 3, length = 1000)
g <- ggplot(data.frame(x = x, y = dnorm(x)),
            aes(x = x, y = y)) + geom_line(size = 2)
g <- g + geom_vline(xintercept = -3 : 3, size = 2)
g
```

* for any $X \sim N(\mu, \sigma^2)$, calculating the number of standard deviations each observation is from the mean ***converts*** the random variable to a ***standard normal*** (denoted as $Z$ below) $$Z=\frac{X-\mu}{\sigma} \sim N(0,1)$$
* conversely, a standard normal can then be converted to ***any normal distribution*** by multiplying by standard deviation and adding the mean $$X = \mu + \sigma Z \sim N(\mu, \sigma^2)$$
* `qnorm(n, mean=mu, sd=sd)` = returns the $n^{th}$ percentiles for the given normal distribution
* `pnorm(x, mean=mu, sd=sd, lower.tail=F)` = returns the probability of an observation drawn from the given distribution is larger in value than the specified threshold $x$

### Example
* the number of daily ad clicks for a company
is (approximately) normally distributed with a mean of 1020 and a standard deviation of 50
* What's the probability of getting more than 1,160 clicks in a day?

```{r}
# calculate number of standard deviations from the mean
(1160 - 1020) / 50
# calculate probability using given distribution
pnorm(1160, mean = 1020, sd = 50, lower.tail = FALSE)
# calculate probability using standard normal
pnorm(2.8, lower.tail = FALSE)
```

* therefore, it is not very likely (`r pnorm(2.8, lower.tail = FALSE)*100`% chance), since 1,160 is `r (1160 - 1020) / 50` standard deviations from the mean
* What number of daily ad clicks would represent the one where 75% of days have fewer clicks (assuming days are independent and identically distributed)?

```{r}
qnorm(0.75, mean = 1020, sd = 50)
```

* therefore, `r qnorm(0.75, mean = 1020, sd = 50)` would represent the threshold that has more clicks than 75% of days




## Poisson Distribution
* used to model counts
	* mean = $\lambda$
	* variance = $\lambda$
	* PMF is defined as $$P(X = x; \lambda)=\frac{\lambda^xe^{-\lambda}}{x!}$$ where $X = 0, 1, 2, ... \infty$
* modeling uses for Poisson distribution
	* count data
	* event-time/survival $\rightarrow$ cancer trials, some patients never develop and some do, dealing with the data for both ("censoring")
	* contingency tables $\rightarrow$ record results for different characteristic measurements
	* approximating binomials $\rightarrow$ instances where **n** is large and **p** is small (i.e. pollution on lung disease)
		* $X \sim Binomial(n, p)$
		* $\lambda = np$
	* rates $\rightarrow$ $X \sim Poisson(\lambda t)$
		* $\lambda = E[X/t]$ $\rightarrow$ expected count per unit of time
		* $t$ = total monitoring time
- `ppois(n, lambda = lambda*t)` = returns probability of $n$ or fewer events happening given the rate $\lambda$ and time $t$

### Example
* number of people that show up at a bus stop can be modeled with Poisson distribution with a mean of 2.5 per hour
* after watching the bus stop for 4 hours, what is the probability that 3 or fewer people show up for the whole time?

```{r}
# calculate using distribution
ppois(3, lambda = 2.5 * 4)
```

* as we can see from above, there is a `r ppois(3, lambda = 2.5 * 4)*100`% chance for 3 or fewer people show up total at the bus stop during 4 hours of monitoring


### Example - Approximating Binomial Distribution
* flip a coin with success probability of 0.01 a total 500 times (low $p$, large $n$)
* what's the probability of 2 or fewer successes?

```{r}
# calculate correct probability from Binomial distribution
pbinom(2, size = 500, prob = .01)
# estimate probability using Poisson distribution
ppois(2, lambda=500 * .01)
```

* as we can see from above, the two probabilities (`r pbinom(2, size = 500, prob = .01)*100`% vs `r pbinom(2, size = 500, prob = .01)*100`%) are extremely close



## Asymptotics
* **asymptotics** = behavior of statistics as sample size $\rightarrow$ $\infty$
* useful for simple statistical inference/approximations
* form basis for frequentist interpretation of probabilities ("Law of Large Numbers")


### Law of Large Numbers (LLN)
* IID sample statistic that estimates property of the sample (i.e. mean, variance) ***becomes*** the population statistic (i.e. population mean, population variance) as $n$ increases
* ***Note**: an estimator is **consistent** if it converges to what it is estimating *
* sample mean/variance/standard deviation are all ***consistent estimators*** for their population counterparts
	- $\bar X_n$ is average of the result of $n$ coin flips (i.e. the sample proportion of heads)
	- as we flip a fair coin over and over, it ***eventually converges*** to the true probability of a head

### Example - LLN for Normal and Bernoulli Distribution
* for this example, we will simulate 10000 samples from the normal and Bernoulli distributions respectively
* we will plot the distribution of sample means as $n$ increases and compare it to the population means

```{r fig.width = 6, fig.height = 3, fig.align = 'center', message = F, warning = F}
# load library
library(gridExtra)
# specify number of trials
n <- 10000
# calculate sample (from normal distribution) means for different size of n
means <- cumsum(rnorm(n)) / (1  : n)
# plot sample size vs sample mean
g <- ggplot(data.frame(x = 1 : n, y = means), aes(x = x, y = y))
g <- g + geom_hline(yintercept = 0) + geom_line(size = 2)
g <- g + labs(x = "Number of obs", y = "Cumulative mean")
g <- g + ggtitle("Normal Distribution")
# calculate sample (coin flips) means for different size of n
means <- cumsum(sample(0 : 1, n , replace = TRUE)) / (1  : n)
# plot sample size vs sample mean
p <- ggplot(data.frame(x = 1 : n, y = means), aes(x = x, y = y))
p <- p + geom_hline(yintercept = 0.5) + geom_line(size = 2)
p <- p + labs(x = "Number of obs", y = "Cumulative mean")
p <- p + ggtitle("Bernoulli Distribution (Coin Flip)")
# combine plots
grid.arrange(g, p, ncol = 2)
```

* as we can see from above, for both distributions the sample means undeniably approach the respective population means as $n$ increases


### Central Limit Theorem
* one of the most important theorems in statistics
* distribution of means of IID variables approaches the standard normal as sample size $n$ increases
* in other words, for large values of $n$, $$\frac{\mbox{Estimate} - \mbox{Mean of Estimate}}{\mbox{Std. Err. of Estimate}} = \frac{\bar X_n - \mu}{\sigma / \sqrt{n}}=\frac{\sqrt n (\bar X_n - \mu)}{\sigma} \longrightarrow N(0, 1)$$
* this translates to the distribution of the sample mean $\bar X_n$ is approximately $N(\mu, \sigma^2/n)$
	- distribution is centered at the population mean
	- with standard deviation = standard error of the mean
* typically the Central Limit Theorem can be applied when $n \geq 30$

### Example - CLT with Bernoulli Trials (Coin Flips)
- for this example, we will simulate $n$ flips of a possibly unfair coin
	- let $X_i$ be the 0 or 1 result of the $i^{th}$ flip of a possibly unfair coin
	+ sample proportion , $\hat p$, is the average of the coin flips
	+ $E[X_i] = p$ and $Var(X_i) = p(1-p)$
	+ standard error of the mean is $SE = \sqrt{p(1-p)/n}$
+ in principle, normalizing the random variable $X_i$, we should get an approximately standard normal distribution $$\frac{\hat p - p}{\sqrt{p(1-p)/n}} \sim N(0,~1)$$
- therefore, we will flip a coin $n$ times, take the sample proportion of heads (successes with probability $p$), subtract off 0.5 (ideal sample proportion) and multiply the result by $\frac{1}{2 \sqrt{n}}$ and compare it to the standard normal

```{r, echo = FALSE, fig.width=6, fig.height = 3, fig.align='center'}
# specify number of simulations
nosim <- 1000
# convert to standard normal
cfunc <- function(x, n) 2 * sqrt(n) * (mean(x) - 0.5)
# simulate data for sample sizes 10, 20, and 30
dat <- data.frame(
	x = c(apply(matrix(sample(0:1, nosim*10, replace=TRUE), nosim), 1, cfunc, 10),
        apply(matrix(sample(0:1, nosim*20, replace=TRUE), nosim), 1, cfunc, 20),
        apply(matrix(sample(0:1, nosim*30, replace=TRUE), nosim), 1, cfunc, 30)),
	size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
# plot histograms for the trials
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3,
	colour = "black", aes(y = ..density..))
# plot standard normal distribution for reference
g <- g + stat_function(fun = dnorm, size = 2)
# plot panel plots by sample size
g + facet_grid(. ~ size)
```

* now, we can run the same simulation trials for an extremely unfair coin with $p$ = 0.9

```{r, echo = FALSE, fig.width=6, fig.height = 3, fig.align='center'}
# specify number of simulations
nosim <- 1000
# convert to standard normal
cfunc <- function(x, n) sqrt(n) * (mean(x) - 0.9) / sqrt(.1 * .9)
# simulate data for sample sizes 10, 20, and 30
dat <- data.frame(
	x = c(apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 10, replace = TRUE),
                     nosim), 1, cfunc, 10),
        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 20, replace = TRUE),
                     nosim), 1, cfunc, 20),
        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 30, replace = TRUE),
                     nosim), 1, cfunc, 30)),
	size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
# plot histograms for the trials
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3,
	colour = "black", aes(y = ..density..))
# plot standard normal distribution for reference
g <- g + stat_function(fun = dnorm, size = 2)
# plot panel plots by sample size
g + facet_grid(. ~ size)
```

* as we can see from both simulations, the converted/standardized distribution of the samples convert to the standard normal distribution
* **Note**: speed at which the normalized coin flips converge to normal distribution depends on how biased the coin is (value of $p$)
* **Note**: does not guarantee that the normal distribution will be a good approximation, but just that eventually it will be a good approximation as n $\rightarrow \infty$
