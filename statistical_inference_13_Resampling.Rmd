---
title: "Statistical Inference Notes"
author: "Howard J, based on the work of Caffo"
date: "December 10, 2017"
output:
html_document:
  highlight: pygments
  theme: spacelab
  toc: yes
header-includes: \usepackage{graphicx}
---


```{r message=FALSE, warning=FALSE}
library(UsingR)
library(tidyverse)
library(grid)
library(png)
```


## The bootstrap
The bootstrap is a tremendously useful tool for constructing confidence intervals and calculating standard errors for difficult statistics. For example, how would one derive a confidence interval for the median.  


### Sample of 50 die rolls

```{r, fig.width=12, fig.height = 6, fig.align='center'}
sim_count <- 1000

df1 <- data.frame(y_val = rep(1/6, 6), x_val = 1:6)
g1 = ggplot(df1, aes(y = y_val, x = x_val))
g1 = g1 + geom_bar(stat = "identity", fill = "lightblue", colour = "black")

df2 <- data.frame(x_val = apply(matrix(sample(1:6, sim_count*50, replace = TRUE), sim_count), 1, mean))
g2 <- ggplot(df2, aes(x = x_val)) + geom_histogram(binwidth=.2, colour = "black", fill = "salmon", aes(y = ..density..)) 

grid.arrange(g1, g2, ncol = 2)

```


### What if we only had one sample?
```{r, fig.width=9, fig.height = 6, fig.align='center'}
sample_size <- 50   
bstrp_count <- 1000 

sample_one <- sample(1:6, sample_size, replace=TRUE)
sample_re <- matrix(sample(sample_one, sample_size*bstrp_count, replace=TRUE), bstrp_count, sample_size)
sample_re_mean <- apply(sample_re, 1, mean)

df1 <- as.data.frame(prop.table(table(sample_one)))
df2 <- data.frame(sample_re_mean)

g1 <- ggplot(df1, aes(x=sample_one, y=Freq)) + geom_bar(colour="black", fill="lightblue", stat="identity") 
g2 <- ggplot(df2, aes(x=sample_re_mean)) + geom_histogram(binwidth=.2, colour="black", fill="salmon", aes(y=..density..)) 

grid.arrange(g1, g2, ncol=2)
```


### Pearson's data set on heights of fathers and their sons    
The code below creates resamples via draws of size bootstrap_count 10000 with replacement with the original data of the son's heights and plots a histogram of the median of each resampled dataset.  
```{r}
data(father.son)
head(father.son)
```

Resamples of father.son ata and calculate median:  
```{r}
height_son <- father.son$sheight
count <- length(height_son)
bstrp_count <- 10000
sample_re <- matrix(sample(height_son, count*bstrp_count, replace=TRUE), bstrp_count, count)
sample_re_med <- apply(sample_re, 1, median)
```

Plot the histogram of father.son resample median:  
```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE, warning=FALSE}
df <- data.frame(sample_re_med)
ggplot(df, aes(x=sample_re_med)) + 
  geom_density(size=2, fill="red") + 
  geom_vline(xintercept=median(height_son), size=2)
#geom_histogram(alpha = .20, binwidth=.3, colour = "black", fill = "blue", aes(y = ..density..)) 

```


### The bootstrap principle  
The bootstrap is a tremendously useful tool for *constructing confidence intervals* and *calculating standard errors* for difficult statistics.  

To illustrate the bootstrap principle, imagine a die roll. The image below shows the mass function of a die roll on the left. On the right we show the empirical distribution obtained by repeatedly averaging 50 independent die rolls. By this simulation, without any mathematics, we have a good idea of what the distribution of averages of 50 die rolls looks like.  

Now imagine a case where we didn't know whether or not the die was fair. We have a sample of size 50 and we'd like to investigate the distribution of the average of 50 die rolls *where we're not allowed to roll the die anymore*. *This is more like a real data analysis, we only get one sample from the population*.  

The __bootstrap principle__ is to *use the empirical mass function of the data to perform the simulation, rather than the true distribution*. That is, we simulate averages of 50 samples from the histogram that we observe. With enough data, the empirical distribution should be a good estimate of the true distribution and this should result in a good approximation of the sampling distribution.

That's the __bootstrap principle__: *investigate the sampling distribution of a statistic by simulating repeated realizations from the observed distribution*.

If we could simulate from the true distribution, then we would know the exact sampling distribution of our statistic (if we ran our computer long enough.) However, since we only get to sample from that distribution once, we have to be content with using the empirical distribution. This is the clever idea of the bootstrap.  



### Nonparametric bootstrap algorithm  
The general procedure: first simulating complete data sets from the observed data with replacement. This is approximately drawing from the sampling distribution of that statistic. Calculate the statistic for each simulated data set. Use the simulated statistics to either define a confidence interval or take the standard deviation to calculate a standard error.  

Bootstrap procedure for calculating confidence interval for the median from a data set of $n$ observations:  
1 Sample $n$ observations **with replacement** from the observed data resulting in one simulated complete data set.  
2 Take the median of the simulated data set.  
3 Repeat these two steps $B$ times, resulting in $B$ simulated medians.  
4 These medians are approximately drawn from the sampling distribution of the median of $n$ observations; therefore we can. Draw a histogram of them. Calculate their standard deviation to estimate the standard error of the median. Take the $2.5^{th}$ and $97.5^{th}$ percentiles as a confidence interval for the median.

*Example: $2.5^{th}$ and $97.5^{th}$ percentiles as a confidence interval for the median*  
```{r}
data(father.son)
height_son <- father.son$sheight
sample_size <- 50
bstrp_count <- 10000
sample_re <- matrix(sample(height_son, sample_size*bstrp_count, replace = TRUE), bstrp_count, sample_size)
sample_re_med <- apply(sample_re, 1, median)
sd(sample_re_med)
quantile(sample_re_med, c(.025, .975))
```

*Histogram: *  
```{r, fig.height=6, fig.width=6, echo=TRUE,fig.align='center', warning=FALSE}
ggplot(data.frame(sample_re_med), aes(x = sample_re_med)) +
  geom_histogram(color = "black", fill = "lightblue", binwidth = 0.05)
```

---

## Notes on the bootstrap
The bootstrap is non-parametric
- Better percentile bootstrap confidence intervals correct for bias
- There are lots of variations on bootstrap procedures; the book "An Introduction to the Bootstrap"" by Efron and Tibshirani is a great place to start for both bootstrap and jackknife information


---
## Group comparisons
- Consider comparing two independent groups.
- Example, comparing sprays B and C

```{r, fig.height=6, fig.width=8, echo=FALSE, fig.align='center'}
data(InsectSprays)
g = ggplot(InsectSprays, aes(spray, count, fill = spray))
g = g + geom_boxplot()
g
```

---
## Permutation tests
-  Consider the null hypothesis that the distribution of the observations from each group is the same
-  Then, the group labels are irrelevant
- Consider a data frome with count and spray
- Permute the spray (group) labels 
- Recalculate the statistic
  - Mean difference in counts
  - Geometric means
  - T statistic
- Calculate the percentage of simulations where
the simulated statistic was more extreme (toward
the alternative) than the observed

---
## Variations on permutation testing
Data type | Statistic | Test name 
---|---|---|
Ranks | rank sum | rank sum test
Binary | hypergeometric prob | Fisher's exact test
Raw data | | ordinary permutation test

- Also, so-called *randomization tests* are exactly permutation tests, with a different motivation.
- For matched data, one can randomize the signs
  - For ranks, this results in the signed rank test
- Permutation strategies work for regression as well
  - Permuting a regressor of interest
- Permutation tests work very well in multivariate settings

---
## Permutation test B v C
```{r}
subdata <- InsectSprays[InsectSprays$spray %in% c("B", "C"),]
y <- subdata$count
group <- as.character(subdata$spray)
testStat <- function(w, g) mean(w[g == "B"]) - mean(w[g == "C"])
observedStat <- testStat(y, group)
permutations <- sapply(1 : 10000, function(i) testStat(y, sample(group)))
observedStat
mean(permutations > observedStat)
```

---
## Histogram of permutations B v C
```{r, echo= FALSE, fig.width=6, fig.height=6, fig.align='center'}
g = ggplot(data.frame(permutations = permutations),
           aes(permutations))
g = g + geom_histogram(fill = "lightblue", color = "black", binwidth = 1)
g = g + geom_vline(xintercept = observedStat, size = 2)
g
```
