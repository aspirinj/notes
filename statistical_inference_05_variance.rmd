---
title: "Statistical Inference Notes"
author: "Howard J, based on the work of Caffo"
date: "December 13, 2017"
output:
html_document:
  highlight: pygments
  theme: spacelab
  toc: yes
header-includes: \usepackage{graphicx}
---

```{r message=FALSE, warning=FALSE}
library(UsingR)
library(tidyverse)
library(grid)
library(png)
```


### The variance
The variance is a measure of *spread*.  
The plot below shows a series of increasing variances.  

```{r, fig.height = 6, fig.width = 8, fig.align='center'}
input_val <- seq(-10, 10, by = .01)
df <- data.frame(
    y_val = c(
        dnorm(input_val, mean = 0, sd = 1),
        dnorm(input_val, mean = 0, sd = 2),
        dnorm(input_val, mean = 0, sd = 3),
        dnorm(input_val, mean = 0, sd = 4)
    ),
    x_val = rep(input_val, 4),
    factor = factor(rep(1 : 4, rep(length(input_val), 4)))
)
df %>%  ggplot(aes(x = x_val, y = y_val, color = factor)) + 
  geom_line(size = 2)    
```

The distribution of averages are always centered at the same spot as the original distribution, but are less spread out. The sample means tend to more converge to population mean than individual observations. (This is why the sample mean is a better estimate than the population mean???)  

If $X$ is a random variable with mean $\mu$, the variance of $X$ is defined as:  
$$Var(X) = E[(X - \mu)^2] = E[X^2] - E[X]^2$$  

The first part of the equation can be considered as the expected squared distance from the mean. The second part is the expansion. The main benefit of working with standard deviations is that they have the same units as the data, whereas the variance has the units squared.  

When talking about variance, we mean the population variance.  



*Example:* What's the variance from the result of a toss of a die?  
$$E[X]^2 = \mu^2 = 3.5^2 = 12.25$$

$$E[X^2] = 1 ^ 2 \times \frac{1}{6} + 2 ^ 2 \times \frac{1}{6} + 3 ^ 2 \times \frac{1}{6} + 4 ^ 2 \times \frac{1}{6} + 5 ^ 2 \times \frac{1}{6} + 6 ^ 2 \times \frac{1}{6} = 15.17$$

$Var(X) = E[X^2] - E[X]^2 \approx 2.92.$


*Example:* What's the variance from the result of the toss of a (potentially biased) coin with probability of heads (1) of $p$?  
$$E[X] = 0 \times (1 - p) + 1 \times p = p$$  

Since $X$ is either 0 or 1:  
$$E[X^2] = E[X] = p$$

Thus:  
$$Var(X) = E[X^2] - E[X]^2 = p - p^2 = p(1 - p)$$  
It's interesting to note that this function is maximized at $p = 0.5$. The plot below shows this by plotting $p(1-p)$ by $p$.  

```{r}
p = seq(0 , 1, length = 1000)
y = p * (1 - p)
plot(p, y, type = "l", lwd = 3, frame = FALSE)
```



## The sample variance
The sample variance is the estimator of the population variance.  
The sample variance is (almost) the average squared deviation of observations around the sample mean. It is given by:  

$$S^2 = \frac{\sum_{i=1} (X_i - \bar X)^2}{n-1}$$  

The sample variance is a random variable, thus it has a distribution and that distribution has an associated population mean. 



## Simulation experiments  
### Simulating from a population with $var=1$  
Notice that these histograms are always centered in the same spot, 1. In other words, the *sample variance is an unbiased estimate of the population variances*. Notice also that they get more concentrated around the 1 as more data goes into them. Thus, sample variances comprised of more observations are less variable than sample variances comprised of fewer.  

*Samples Variance*, samples from a population with standard normal distribution. The sample size of the salmon color distribution is 10. The simulation count is 10000.     
```{r, fig.height=6, figh.width=6, fig.align='center', echo = FALSE}
sim_count <- 10000 
df <- data.frame(
    sim_val = c(apply(matrix(rnorm(sim_count*10), sim_count), 1, var),
          apply(matrix(rnorm(sim_count*20), sim_count), 1, var),
          apply(matrix(rnorm(sim_count*30), sim_count), 1, var)),
    sample_size = factor(rep(c("10", "20", "30"), c(sim_count, sim_count, sim_count))) 
    )
ggplot(df, aes(x = sim_val, fill = sample_size)) + 
  geom_density(size = 2, alpha = .2) + 
  geom_vline(xintercept = 1, size = 2) 
```


### Variances of x die rolls
Let's try the same thing, now only with die rolls instead of simulating standard
normals. In this experiment, we simulated samples of die rolls, took the
variance and then repeated that process over and over. What is plotted
are histograms of the collections of sample variances.

Sample Variance, sam
```{r, fig.align='center',fig.height=5, fig.width=10, echo = FALSE, warning=FALSE, error=FALSE, message=FALSE}  
sim_count <- 10000
df <- data.frame(
  sim_val = c(apply(matrix(sample(1:6, sim_count*10, replace = TRUE), sim_count), 1, var),
              apply(matrix(sample(1:6, sim_count*20, replace = TRUE), sim_count), 1, var),
              apply(matrix(sample(1:6, sim_count*30, replace = TRUE), sim_count), 1, var)
        ),
  sample_size = factor(rep(c(10, 20, 30), rep(sim_count, 3))))
ggplot(df, aes(x = sim_val, fill = sample_size)) + 
  geom_histogram(alpha = .20, binwidth=.3, colour = "black") + 
  geom_vline(xintercept = 2.92, size = 2) + 
  facet_grid(. ~ sample_size)
```

Recall that we calculated the variance of a die roll as 2.92 earlier on in
this chapter. Notice each of the histograms are centered there. In addition,
they get more concentrated around 2.92 as more the variances are comprised
of more dice.


### The standard error of the mean  
How to estimate the variability of the mean of a sample, when we only get to observe one realization. The average of random sample from a population is itself a random variable having a distribution. We know that this distribution is centered around the population mean, $E[\bar X] = \mu$. We also know the variance of the distribution of means of random samples: $Var(\bar X) = \sigma^2 / n$, where $\sigma^2$ is the variance of the population being sampled from.

So, we can get a estimate of the variability of the mean, even though we only get to observe 1 mean. Notice also this explains why in all of our simulation experiments the variance of the sample mean kept getting smaller as the sample size increased.

The standard deviation of a statistic its is __standard error__.

### Summary notes
* The sample variance, $S^2$, estimates the population variance, $\sigma^2$.
* The distribution of the sample variance is centered around $\sigma^2$.
* The variance of the sample mean is $\sigma^2 / n$.
  * Its logical estimate is $s^2 / n$.
  * The logical estimate of the standard error is $S / \sqrt{n}$.
* $S$, the standard deviation, talks about how variable the population is.
* $S/\sqrt{n}$, the standard error, talks about how variable averages of random
samples of size $n$ from the population are.

### Simulation example 1: standard normals

[Watch this video before beginning.](http://youtu.be/uPjHB9JjGKI?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ)

Standard normals have variance 1. Let's try sampling
means of $n$ standard normals. If our theory is correct, they should
 have standard deviation $1/\sqrt{n}$


{title="Simulating means of random normals", line-numbers=off,lang=r}
~~~
> sim_count <- 1000
> n <- 10
## simulate sim_count averages of 10 standard normals
> sd(apply(matrix(rnorm(sim_count * n), sim_count), 1, mean))
[1] 0.3156
## Let's check to make sure that this is sigma / sqrt(n)
> 1 / sqrt(n)
[1] 0.3162
~~~

So, in this simulation, we simulated 1000 means of 10 standard normals. Our
theory says the standard deviation of averages of 10 standard normals must
be $1/\sqrt{n}$. Taking the standard deviation of the 10000 means yields
nearly exactly that. (Note that it's only close, 0.3156 versus 0.31632.
 To get it to be exact, we'd have to simulate
infinitely many means.)

### Simulation example 2: uniform density
Standard uniforms have variance $1/12$. Our theory mandates
that means of random samples of $n$ uniforms
have sd $1/\sqrt{12 \times n}$. Let's try it with a simulation.



{title="Simulating means of uniforms", line-numbers=off,lang=r}
~~~
> sim_count <- 1000
> n <- 10
> sd(apply(matrix(runif(sim_count * n), sim_count), 1, mean))
[1] 0.09017
> 1 / sqrt(12 * n)
[1] 0.09129
~~~

### Simulation example 3: Poisson
Poisson(4) random variables have variance $4$. Thus means of
random samples of $n$ Poisson(4)
should have standard deviation $2/\sqrt{n}$. Again let's try it out.

{title="Simulating means of Poisson variates", line-numbers=off,lang=r}
~~~
> sim_count <- 1000
> n <- 10
> sd(apply(matrix(rpois(sim_count * n, 4), sim_count), 1, mean))
[1] 0.6219
> 2 / sqrt(n)
[1] 0.6325
~~~

### Simulation example 4: coin flips
Our last example is an important one. Recall that the variance of a
coin flip is $p (1 - p)$. Therefore the standard deviation of the average
of $n$ coin flips should be $\sqrt{\frac{p(1-p)}{n}}$.

Let's just do the simulation with a fair coin. Such coin
flips have variance 0.25. Thus means of
random samples of $n$ coin flips have sd $1 / (2 \sqrt{n})$.
Let's try it.



{title="Simulating means of coin flips", line-numbers=off,lang=r}
~~~
> sim_count <- 1000
> n <- 10
> sd(apply(matrix(sample(0 : 1, sim_count * n, replace = TRUE),
                sim_count), 1, mean))
[1] 0.1587
> 1 / (2 * sqrt(n))
[1] 0.1581
~~~

## Data example
[Watch this before beginning.](http://youtu.be/Lm2DMVyZVxk?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ)

Now let's work through a data example to show how the standard error of the
mean is used in practice. We'll use the `father.son` height data from Francis
Galton.

{title="Loading the data", line-numbers=off,lang=r}
~~~
library(UsingR); data(father.son);
x <- father.son$sheight
n<-length(x)
~~~

Here's a histogram of the sons' heights from the dataset.
Let' calculate different variances and interpret them in this context.

![Histogram of the sons' heights](images/fatherSon.png)


{title="Loading the data", line-numbers=off,lang=r}
~~~
>round(c(var(x), var(x) / n, sd(x), sd(x) / sqrt(n)),2)
[1] 7.92 0.01 2.81 0.09
~~~

The first number, 7.92, and its square root, 2.81, are the estimated variance
and standard deviation of the sons' heights. Therefore, 7.92 tells us exactly how variable
sons' heights were in the data and estimates how variable sons' heights are
in the population. In contrast 0.01, and the square root 0.09, estimate how
variable averages of $n$ sons' heights are.

Therefore, the smaller numbers discuss the precision of our estimate of the mean
of sons' heights. The larger numbers discuss how variable sons' heights are in general.

## Summary notes
* The sample variance estimates the population variance.
* The distribution of the sample variance is centered at
what its estimating.
* It gets more concentrated around the population variance with larger sample sizes.
* The variance of the sample mean is the population variance
divided by $n$.
  * The square root is the standard error.
* It turns out that we can say a lot about the distribution of
averages from random samples,
even though we only get one to look at in a given data set.

