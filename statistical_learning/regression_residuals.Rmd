---
title: "Regression - Residuals"
author: "Howard J"
date: "December 28, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(UsingR)
library(tidyverse)
```

## Residuals

### Residual variation
SLR (Simple Linear Regression) model: 
$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$  
where, $\epsilon_i$ is iid Gaussian errors:  
$$\epsilon_i \sim N(0, \sigma^2)$$

predicted outcome:  
$$\hat Y_i = \hat \beta_0 + \hat \beta_1 X_i$$  
Where, predictor value: $X_i$, Observed outcome: $Y_i$  


The residual is defined as the difference the between the *observed* and *predicted* outcome:  
$$e_i = Y_i - \hat Y_i$$

Least squares minimizes the sum of the squared residuals: $\sum_{i=1}^n e_i^2$    

**Residuals** represent variation left unexplained by our model. *Geometrical interpretation of residuals*: the vertical distance between the *observed* data $Y_i$ point and the associated *predicted* $\hat Y_i$ point on the regression line. Positive residuals points are above the fitted line and negative residuals below.  

**Errors**: *unobservable* true errors from the known coefficients.  
**Residuals**: *observable* errors from the estimated coefficients. In a sense, *the residuals are estimates of the errors*.  


### Properties
* Under our model, their expected value is 0,  $E[e_i] = 0$. 
* The residuals are not independent, since $\sum_{i=1}^n e_i = 0$, if an intercept is included. If we know $n-1$ of them,we know the $n^{th}$. In fact, we will only have $n-p$ free residuals, where $p$ is the number of coefficients in our regression model, so $p=2$ for linear regression with an intercept and slope. If a regressor variable, $X_i$, is included in the model then $\sum_{i=1}^n e_i X_i = 0$.


### Usage
* The residuals are useful for investigating poor model fit. 
* To create covariate adjusted variables. Specifically, residuals can be thought of as the outcome (Y) with the linear association of the predictor (X) removed. 
So, for example, if you wanted to create a weight variable with the linear effect of height removed, you would fit a linear regression with weight as the outcome and height as the predictor and take the residuals. (Note this only works if the relationship is linear.)  


### `diamond` Data Set
Data is diamond prices (Singapore dollars) and diamond weight in carats (0.2 $g$).  

#### Plot data and regression line
```{r,fig.height=6,fig.width=6}
data(diamond)
ggplot(diamond, aes(x = carat, y = price)) + 
    xlab("Mass (carats)") +
    ylab("Price ($)") +
    geom_smooth(method = "lm", colour = "black") + 
    geom_point(size = 7, colour = "black", alpha=0.5) +
    geom_point(size = 5, colour = "blue", alpha=0.2)
```

#### 
```{r}
data(diamond)
y <- diamond$price 
x <- diamond$carat
n <- length(y)

fit <- lm(y ~ x)
e <- resid(fit) # e is the residuals directly from linear model
yhat <- predict(fit)

max(abs(e -(y - yhat)))
max(abs(e - (y - coef(fit)[1] - coef(fit)[2] * x)))
```


Residuals are the signed length of the red lines:  
```{r, fig.height=5, fig.width=5}
data(diamond)
y <- diamond$price 
x <- diamond$carat
n <- length(y)

fit <- lm(y ~ x)

plot(x, y,  
     xlab = "Mass (carats)",
     ylab = "Price (SIN $)",
     bg = "lightblue",
     col = "black", cex = 2, pch = 21,frame = FALSE)
abline(fit, lwd = 2)
for (i in 1 : n)
  lines(c(x[i], x[i]), c(y[i], yhat[i]), col = "red" , lwd = 2)
```








Residuals versus X
```{r, fig.height=5, fig.width=5}
data(diamond)
y <- diamond$price 
x <- diamond$carat
n <- length(y)

fit <- lm(y ~ x)

plot(x, e,  
     xlab = "Mass (carats)",
     ylab = "Residuals (SIN $)",
     bg = "lightblue",
     col = "black", cex = 2, pch = 21,frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n)
  lines(c(x[i], x[i]), c(e[i], 0), col = "red" , lwd = 2)
```