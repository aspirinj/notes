---
title: "Statistical Inference Notes"
author: "Howard J"
date: "December 10, 2017"
output:
html_document:
  highlight: pygments
  theme: spacelab
  toc: yes
header-includes: \usepackage{graphicx}
---

```{r message=FALSE, warning=FALSE}
library(UsingR)
library(tidyverse)
library(grid)
library(png)
```

## Overview
* **Statistical Inference** = generating conclusions about a population from a noisy sample
* Goal = extend beyond data to population
* Statistical Inference = only formal system of inference we have
* many different modes, but **two** broad flavors of inference (inferential paradigms): ***Bayesian*** vs ***Frequencist***
    * **Frequencist** = uses long run proportion of times an event occurs independent identically distributed repetitions
		* frequentist is what this class is focused on
		* believes if an experiment is repeated many many times, the resultant percentage of success/something happening defines that population parameter
	* **Bayesian** = probability estimate for a hypothesis is updated as additional evidence is acquired
* **statistic** = number computed from a sample of data
	* statistics are used to infer information about a population
* **random variable** = outcome from an experiment
	* deterministic processes (variance/means) produce additional random variables when applied to random variables, and they have their own distributions


## Probability  
__Randomnes__ is any process occurring without *apparent* deterministic patterns.  
*Example:* We will treat many things as if they were random when, in fact they are completely deterministic. In my field, biostatistics, we often model disease outcomes as if they were random when they are the result of many mechanistic
components whose aggregate behavior appears random.  

__Probability__ of an event is in terms of its *relative frequency*.  
We suppose that an experiment, whose sample space is $S$, is repeatedly performed under exactly the same conditions. For each event $E$ of the sample space $S$, we define $n(E)$ to be the number of times in the first $n$ repetitions of the experiment that the event $E$ occurs. Then $P(E)$, the probability of the event $E$, is defined as:  
$$P(E)=\lim_{n\rightarrow\infty}{n(E) \over n}$$

Given a random experiment, a *__probability measure__ is a __population__ quantity that summarizes the randomness*.
Notice that, *it is not about the data we have, but a conceptual quantity that exist in the population* that we want to estimate

## Random Variables
__Random variable__ is the numeric outcome of experiment.  
__Discrete__, assign probabilities to *every* number/value the variable can take.  
__Continuous__, assign probabilities to the *range* the variable can take.  
*Note:* limitations of precision in taking the measurements may imply that the values are discrete, but we in fact consider them continuous.  
* Density function and Mass function (population quantities, not what occurs in data) for random variables = best starting point to model/think about probabilities for numeric outcome of experiments (variables)
* use data to estimate properties of population $\rightarrow$ linking sample to population  


### Probability Calculus    
* __*Probability* is a function of any set of outcomes and assigns it a number between 0 and 1__.  $0 \le P(E) \le 1$, where $E$ means __*Event*__.
* *A measure of certainty*, 0 is impossible and 1 is certain.
* The probability of an Event $E$ is shown as $P(E)$. The ratio of the number of times of $E$ occurring compared to the number of all outcomes.
* The union of independent events $A$ and $B$, $P(A  \cup  B) = P(A) \times P(B)$


### Kolmogorovâ€™s Three Rules
Given a random experiment (say rolling a die) a probability measure is a *population quantity* that summarizes the randomness.  
Consider an experiment with a random outcome. Probability takes a possible outcome from an experiment and:  
1. $0 \le P(E) \le 1$
2. $P(S)=1$  
3. $P(\cup_{i=1}^\infty E) = \sum_{i=1}^\infty P(E)$, required that the probability of the union of any two sets of outcomes that have nothing in common (mutually exclusive) is the sum of their respective probabilities


### Distribution  

* For each distribution, R has four functions:  

Prefix  |  Continous  |  Discrete
--------|-------------|-----------
d  | density | PMF
p  | CDF | CDF
q  | quantile | quantile
r  | random | random

* Distribution root name:  

Distribution | Root
-------------|-----
Binomial     | binom
Poisson      | pois
Normal       | norm  
t            | t  
F            | F  
Chi-square   | chisq  


### Binomial Distribution  
The __Binomial Distribution__ model deals with finding the *probability of success of an event which has only two possible outcomes in a series of experiments*. For example, tossing of a coin always gives a head or a tail. The probability of finding exactly 3 heads in tossing a coin repeatedly for 10 times is estimated during the binomial distribution.  

Four functions are provided by R:  
`dbinom(x, size, prob)`  
`pbinom(q, size, prob)`  
`qbinom(p, size, prob)`  
`rbinom(n, size, prob)`  

`x, q`: vector of quantiles (input vector of integers), e.g. number of heads.  
`p` is a vector of probabilities.  
`n` is number of observations.  
`size` is the number of trials.  
`prob` is the probability of success of each trial, e.g. the probability for head of each trial.  

__*dbinom*__  
`dbinom` returns binomial probabilities. The returned value is PMF values at the input vector of integers.  
```{r dbinom}
# Create a sample of 50 numbers which are incremented by 1.
x <- seq(0,50,by = 1)
# Create the binomial distribution.
y <- dbinom(x,50,0.5)
plot(x,y)
```

__*pbinom*__  
`pbinom(x, size, prob)` gives the cumulative probability distribution of an event. It is a single value representing the cumulative probability.  
*Example:* Probability of getting 26 or less heads from a 51 tosses of a coin.    
```{r pbinom}
x <- pbinom(26,51,0.5)
print(x)
```

*Example:* the cumulative probability distribution for the previous example.  
```{r pbinom2}
x <- seq(0,50,by = 1)
y <- pbinom(x,50,0.5)
plot(x,y)
```


__*qbinom*__  
`qbinom` takes the cumulative probability value and gives a number which matches this cumulative probability value.  
*Example:* how many heads will have a probability of success of 0.25 when a coin is tossed 51 times:  
```{r qbinom}
x <- qbinom(0.25,51,1/2)
print(x)
```

__*rbinom*__  
`rbinom` generates required number of random values of given probability from a given sample.  
*Example:* generate 8 random values (each value is the number of success, e.g. heads) from a sample of 150 with probability of 0.4:  
```{r rbinom}
x <- rbinom(8,150,.4)
print(x)
```



### PMF (Probability Mass Function)  
* A __PMF__ evaluates the probability that the __Discrete Random Variable__ takes on a specific value.  
* Always be $\ge$ 0 for every possible outcome.
* The sum of the possible values that the random variable can take has to add up to one.

* *Bernoulli Distribution example:* Let $X$ be the result of a coin flip.  
  - $X = 0 \rightarrow tails$, $X = 1 \rightarrow heads$  
  - $P(X = x) = (\frac{1}{2})^x(\frac{1}{2})^{1-x}$ for $X = 0, 1$, where $x$ here represents a value we can plug into the PMF.  
  - A general form is $p(x) = (\theta)^x(1-\theta)^{1-x}$.  
*Example:* `dbinom(x, size, prob)` = return the probability of getting `x` successes out of `size` trials, given probability of success is `prob`.



### PDF (Probability Density Function)  
* The central dogma of __PDF__: Areas under PDFs correspond to probabilities for that random variable.  
* Two conditions to justify a mathematically valid density:  
  - Always $\ge0$  everywhere
  - Total area under the whole curve must $=1$
* **areas under PDFs** correspond to the probabilities for that random variable taking on that range of values (PMF)
* *Example:* when one says that intelligence quotients (IQ) in population follows a bell curve, they are saying that the *probability of a randomly selected person from this population having an IQ between two values is given by the area under the bell curve*.
* *Example:* suppose that the proportion of help calls that get addressed in a random day by a help line is given by $f(x) = 2x$ for $0 < x < 1$. What is the probability that 75% or fewer of calls get addressed?
* The probability is the area: ${1.5 \times 0.75 \over 2} = 0.5625$  

```{r}
x <- c(-0.5, 0, 1, 1, 1.5)
y <- c(0, 0, 2, 0, 0)
plot(x, y, lwd = 3,frame = FALSE, type = "l")
```
This special density is the beta density:  
```{r}
pbeta(0.75, 2, 1)
```


### Cumulative Distribution Function (CDF)  
* The __CDF__ of a Random Variable, $X$, returns the probability that the random variable is less than or equal to the value $x$. The distribution function $F$:
$$F(x) = P(X \le x)$$   
*This definition applies regardless of whether the random variable is discrete or continuous.*  
* *Notice* the convention that we use an upper case $X$ to denote an unrealized random variable and a lowercase $x$ to denote a specific number that we plug into.  
* PDF = derivative of CDF
	* integrate PDF $\rightarrow$ CDF
		* `integrate(function, lower=0, upper=1)` $\rightarrow$ can be used to evaluate integrals for a specified range
* `pbinom()`, `pnorm()`, `pgamma()`, `ppois()`, `punif()` = returns the cumulative probabilities from 0 up to a specified value from the binomial, normal, Gamma, Poisson, and uniform distributions


### Survival Function
* The __Survival Function__ of a random variable $X$ is defined as the probability the random variable is greater than the value $x$. It is the complement of CDF.
$$S(x) = P(X > x) = 1 - F(x)$$
where $F(x) = CDF$.  
* *Example:* What are the survival function and CDF from the density considered before?
$$S(x) = 1 - F(x) = 1 - x^2$$
```{r}
1 - pbeta(c(0.4, 0.5, 0.6), 2, 1)
```


### Quantile
* the $\alpha^{th}$ quantile of a distribution with distribution function F = point $x_{\alpha}$
	* $F(x_{\alpha}) = \alpha$
	* percentile = quantile with $\alpha$ expressed as a percent
	* median = 50^th^ percentile
	* $\alpha$% of the possible outcomes lie below it

```{r echo = FALSE, fig.width = 3, fig.height = 3, fig.align = 'center'}
grid.raster(readPNG("figures/5.png"))
```

* `qbeta(quantileInDecimals, 2, 1)` = returns quantiles for beta distribution
    * works for `qnorm()`, `qbinom()`, `qgamma()`, `qpois()`, etc.
* median estimated in this fashion = a population median
* probability model connects data to population using assumptions
	* population median = ***estimand***, sample median = ***estimator***





  ### Conditional Probability  
This is the idea of conditioning, *taking away the randomness* that we know to have occurred.  
The __Conditional probability__ of an event $A$ given that event $B$ has occurred:  
$$P(A | B) = \frac{P(A  \cap  B)}{P(B)}$$
If $A$ and $B$ are *unrelated*, in other words __Independent__, then:  
$$P(A | B) = \frac{P(A)P(B)}{P(B)} = P(A)$$
  * *Example:* for die roll, $A = \{1\}$, $B = \{1, 3, 5\}$. Given that we already know the die roll was an odd number, the conditional probability now is $1\over3$, because we have extra information.  
  $$P(A | B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A)}{P(B)} = \frac{1/6}{3/6} = \frac{1}{3}$$

  ### Baye's Rule
  * definition $$P(B | A) = \frac{P(A | B)P(B)}{P(A | B)P(B)+P(A | B^c)P(B^c)}$$ where $B^c$ = corresponding probability of event $B$, $P(B^c) = 1 - P(B)$




### Independence
Two events $A$ and $B$ are __Independent__ if the following is true  
$$P(A \cap B) = P(A)P(B)$$  
Or equivalently by using the definition of conditional probability:  
$$P(A | B) = P(A)$$  
*While this definition works for __sets__, remember that __random variables__ are really the things that we are interested in.* Two random variables $X$ and $Y$ are independent, if for any two sets, $A$ and $B$:  
$$P([X \in A]\cap[Y \in B]) = P(X \in A)P(Y \in B)$$  


__Mutually Exclusive__ (__Disjoint__) Events are events that cannot both happen at the same time: if $A$ and $B$ are mutually exclusive. $P(A \cap B)=0$  
  Distinguish between __Disjoint__ and __Independent__ events:  
  * If $A$ and $B$ are independent, then having information on $A$ does not tell us anything about $B$ (and vice versa).
  * If $A$ and $B$ are disjoint, then knowing that $A$ occurs tells us that $B$ cannot occur (and vice versa).
  * *Disjoint (mutually exclusive) events are always dependent* since if one event occurs we know the other one cannot.
  *

### IID Random Variables
* random variables are said to be **IID** if they are ***independent and identically distributed***
	* **independent** = statistically unrelated from each other
	* **identically distributed** = all having been drawn from the same population distribution
* IID random variables = default model for random samples = default starting point of inference



## Diagnostic Test
* Let $+$ and $-$ be the results, positive and negative respectively, of a diagnostic test
* Let $D$ = subject of the test has the disease, $D^c$ = subject does not
* **sensitivity** = $P(+ | D)$ = probability that the test is positive given that the subject has the disease (the higher the better)
* **specificity** = $P(- | D^c)$ = probability that the test is negative given that the subject does not have the disease (the higher the better)
* **positive predictive value** = $P(D | +)$ = probability that that subject has the disease given that the test is positive
* **negative predictive value** = $P(D^c | -)$ = probability that the subject does not have the disease given the test is negative
* **prevalence of disease** = $P(D)$ = marginal probability of disease

### Example
* specificity of 98.5%, sensitivity = 99.7%, prevalence of disease = .1%
$$\begin{aligned}
P(D ~|~ +) & = \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}\\
& = \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + \{1-P(-~|~D^c)\}\{1 - P(D)\}} \\
& = \frac{.997\times .001}{.997 \times .001 + .015 \times .999}\\
& =  .062
\end{aligned}$$
* low positive predictive value $\rightarrow$ due to low prevalence of disease and somewhat modest specificity
	* suppose it was know that the subject uses drugs and has regular intercourse with an HIV infect partner (his probability of being + is higher than suspected)
	* evidence implied by a positive test result


### Likelihood Ratios
* **diagnostic likelihood ratio** of a **positive** test result is defined as $$DLR_+ = \frac{sensitivity}{1-specificity} =  \frac{P(+ | D)}{P(+ | D^c)}$$
* **diagnostic likelihood ratio** of a **negative** test result is defined as $$DLR_- = \frac{1 - sensitivity}{specificity} =  \frac{P(- | D)}{P(- | D^c)}$$
* from Baye's Rules, we can derive the *positive predictive value* and *false positive value*
$$P(D | +) = \frac{P(+ | D)P(D)}{P(+ | D)P(D)+P(+ | D^c)P(D^c)}~~~~~~\mbox{(1)}$$
$$P(D^c | +) = \frac{P(+ | D^c)P(D^c)}{P(+ | D)P(D)+P(+ | D^c)P(D^c)}~~~~~~\mbox{(2)}$$
* if we divide equation $(1)$ over $(2)$, the quantities over have the same denominator so we get the following $$\frac{P(D | +)}{P(D^c | +)} = \frac{P(+ | D)}{P(+ | D^c)} \times \frac{P(D)}{P(D^c)}$$ which can also be written as $$\mbox{post-test odds of D} = DLR_+ \times \mbox{pre-test odds of D}$$
	* **odds** = $p/(1-p)$
	* $\frac{P(D)}{P(D^c)}$ = **pre-test odds**, or odds of disease in absence of test
	* $\frac{P(D | +)}{P(+ | D^c)}$ = **post-test odds**, or odds of disease given a positive test result
	* $DLR_+$ = factor by which the odds in the presence of a positive test can be multiplied to obtain the post-test odds
	* $DLR_-$ = relates the decrease in odds of disease after a negative result
* following the previous example, for sensitivity of 0.997 and specificity of 0.985, so the diagnostic likelihood ratios are as follows $$DLR_+ = .997/(1-.985) = 66 ~~~~~~ DLR_- =(1-.997)/.985 = 0.003$$
	* this indicates that the result of the positive test is the odds of disease is 66 times the pretest odds





## Expected Values
* __Expected Values__, including __Mean__, __Variance__, __Skewness__, etc. characterize a distribution. They are the properties of distributions.  
* __Mean__ characterizes of the center of a density or mass function.
 the distribution = *expected value*
* expected value operation = ***linear*** $\rightarrow$ $E(aX +bY) = aE(X) + bE(Y)$
* __Variance__ characterizes how spread out the distribution is.
* __Skewness__ considers how much a density is pulled toward high or low values.
* *Note:in this part we are discussing __Population__ quantities*
* _sample_ expected values for sample mean and variance will estimate the _population_ counterparts

* **population mean**
	* expected value/mean of a random variable = center of its distribution (center of mass)
	* ***discrete variables***
		* for $X$ with PMF $p(x)$, the population mean is defined as $$E[X] = \sum_{x} xp(x)$$ where the sum is taken over ***all*** possible values of $x$
		* $E[X]$ = center of mass of a collection of location and weights ${x,~p(x)}$
        * *coin flip example*: $E[X] = 0 \times (1-p) + 1 \times p = p$
	* ***continuous variable***
		* for $X$ with PDF $f(x)$, the expected value = the center of mass of the density
		* instead of summing over discrete values, the expectation ***integrates*** over a continuous function
			* PDF = $f(x)$
			* $\int xf(x)$ = area under the PDF curve = mean/expected value of $X$

* **sample mean**
	* sample mean estimates the population mean
		* sample mean = center of mass of observed data = empirical mean
        $$\bar X = \sum_{x}^n x_i p(x_i)$$ where $p(x_i) = 1/n$

```{r fig.width = 5, fig.height = 3, fig.align = 'center', message = F, warning = F}
# load relevant packages
library(UsingR); data(galton); library(ggplot2)
# plot galton data
g <- ggplot(galton, aes(x = child))
# add histogram for children data
g <- g + geom_histogram(fill = "salmon", binwidth=1, aes(y=..density..), colour="black")
# add density smooth
g <- g + geom_density(size = 2)
# add vertical line
g <- g + geom_vline(xintercept = mean(galton$child), size = 2)
# print graph
g
```


* **average of random variables** = a new random variable where its distribution has an expected value that is the **same** as the original distribution (centers are the same)
	* the mean of the averages = average of the original data $\rightarrow$ estimates average of the population
	* if $E[sample~mean]$ = population mean, then estimator for the sample mean is **unbiased**
		* [**derivation**] let $X_1$, $X_2$, $X_3$, ... $X_n$ be a collection of $n$ samples from the population with mean $\mu$
		* mean of this sample $$\bar X = \frac{X_1 + X_2 + X_3 +  .  + X_n}{n}$$
		* since $E(aX) = aE(X)$, the expected value of the mean is can be written as $$E\left[\frac{X_1 + X_2 + X_3 + ... + X_n}{n}\right] = \frac{1}{n} \times [E(X_1) + E(X_2) + E(X_3) + ... + E(X_n)]$$
		* since each of the $E(X_i)$ is drawn from the population with mean $\mu$, the expected value of each sample should be $$E(X_i) = \mu$$
		* therefore $$\begin{aligned}
E\left[\frac{X_1 + X_2 + X_3 + ... + X_n}{n}\right] & = \frac{1}{n} \times [E(X_1) + E(X_2) + E(X_3) + ... + E(X_n)]\\
& = \frac{1}{n} \times [\mu + \mu + \mu + ... + \mu]\\
& = \frac{1}{n} \times n \times \mu\\
& = \mu\\
\end{aligned}$$
* **Note**: the more data that goes into the sample mean, the more concentrated its density/mass functions are around the population mean *

```{r fig.width = 6, fig.height = 3, fig.align = 'center', message = F, warning = F}
nosim <- 1000
# simulate data for sample size 1 to 4
dat <- data.frame(
	x = c(sample(1 : 6, nosim, replace = TRUE),
        apply(matrix(sample(1 : 6, nosim * 2, replace = TRUE), nosim), 1, mean),
        apply(matrix(sample(1 : 6, nosim * 3, replace = TRUE), nosim), 1, mean),
        apply(matrix(sample(1 : 6, nosim * 4, replace = TRUE), nosim), 1, mean)),
	size = factor(rep(1 : 4, rep(nosim, 4))))
# plot histograms of means by sample size
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.25, colour = "black")
g + facet_grid(. ~ size)
```
