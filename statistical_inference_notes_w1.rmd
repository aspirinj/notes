---
title: "Statistical Inference Notes"
author: "Howard J, base on the work of Xing Su"
date: "December 10, 2017"
output:
html_document:
  highlight: pygments
  theme: spacelab
  toc: yes
header-includes: \usepackage{graphicx}
---

## Overview
* **Statistical Inference** = generating conclusions about a population from a noisy sample
* Goal = extend beyond data to population
* Statistical Inference = only formal system of inference we have
* many different modes, but **two** broad flavors of inference (inferential paradigms): ***Bayesian*** vs ***Frequencist***
    * **Frequencist** = uses long run proportion of times an event occurs independent identically distributed repetitions
		* frequentist is what this class is focused on
		* believes if an experiment is repeated many many times, the resultant percentage of success/something happening defines that population parameter
	* **Bayesian** = probability estimate for a hypothesis is updated as additional evidence is acquired
* **statistic** = number computed from a sample of data
	* statistics are used to infer information about a population
* **random variable** = outcome from an experiment
	* deterministic processes (variance/means) produce additional random variables when applied to random variables, and they have their own distributions


## Probability  
__*Definition*__  
Given a random experiment, a __*probability measure* is a *population* quantity that summarizes the randomness__.
Notice that, *it is not about the data we have, but a conceptual quantity that exist in the population* that we want to estimate

## Random Variables
* __Random variable__ is the numeric outcome of experiment.  
* __Discrete__, assign probabilities to *every* number/value the variable can take.  
* __Continuous__, assign probabilities to the *range* the variable can take.
* Note: limitations of precision in taking the measurements may imply that the values are discrete, but we in fact consider them continuous
  - Functions `rbinom()`, `rnorm()`, `rgamma()`, `rpois()`, `runif()` are used to generate Random Variables from the binomial, normal, Gamma, Poisson, and uniform distributions.  
  - Density function and Mass function (population quantities, not what occurs in data) for random variables = best starting point to model/think about probabilities for numeric outcome of experiments (variables)
  - use data to estimate properties of population $\rightarrow$ linking sample to population
* __Mutually Exclusive__ (__Disjoint__) Events are events that cannot both happen at the same time: if $A$ and $B$ are mutually exclusive. $P(A\:\cap\:B)=0$
* Distinguish between __Disjoint__ and __Independent__ events:  
  - If $A$ and $B$ are independent, then having information on $A$ does not tell us anything about $B$ (and vice versa).
  - If $A$ and $B$ are disjoint, then knowing that $A$ occurs tells us that $B$ cannot occur (and vice versa).
  - *Disjoint (mutually exclusive) events are always dependent* since if one event occurs we know the other one cannot.

### General Probability Rules
* Also known as "Probability Calculus"  
* __*Probability* is a function of any set of outcomes and assigns it a number between 0 and 1__.  $0 \le P(E) \le 1$, where $E$ means __*Event*__.
* A measure of certainty, 0 is impossible and 1 is certain.
* The probability of an Event $E$ is shown as $P(E)$. The ratio of the number of times of $E$ occurring compared to the number of all outcomes.
* The union of independent events $A$ and $B$, $P(A \:\cup\: B) = P(A) \times P(B)$

### Conditional Probability
* The __Conditional probability__ of an event $A$ given that event $B$ has occurred:  
$$P(A\:|\:B) = \frac{P(A \:\cap\: B)}{P(B)}$$
* If $A$ and $B$ are unrelated, in other words __Independent__, then:  
$$P(A\:|\:B) = \frac{P(A)P(B)}{P(B)} = P(A)$$
* *Example:* for die roll, $A = \{1\}$, $B = \{1, 3, 5\}$. Given that we already know the die roll was an odd number, the conditional probability now is $1\over3$, because we have extra information.  
$$P(A\:|\:B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A)}{P(B)} = \frac{1/6}{3/6} = \frac{1}{3}$$

### Baye's Rule
* definition $$P(B\:|\:A) = \frac{P(A\:|\:B)P(B)}{P(A\:|\:B)P(B)+P(A\:|\:B^c)P(B^c)}$$ where $B^c$ = corresponding probability of event $B$, $P(B^c) = 1 - P(B)$


### Distribution  

* For each distribution, R has four functions:  

Prefix  |  Continous  |  Discrete
--------|-------------|-----------
d  | density | PMF
p  | CDF | CDF
q  | quantile | quantile
r  | random | random

* Distribution root name:  

Distribution | Root
-------------|-----
Binomial     | binom
Poisson      | pois
Normal       | norm  
t            | t  
F            | F  
Chi-square   | chisq  




### PMF (Probability Mass Function)  
* A __PMF__ evaluates the probability that the __Discrete Random Variable__ takes on a specific value.  
	- It measures the chance of a particular outcome happening
  - It must always be $\ge$ 0 for every possible outcome.
  - The sum of the possible values that the random variable can take has to add up to one.

* *Bernoulli Distribution example:* Let $X$ be the result of a coin flip.  
  - $X = 0 \rightarrow tails$, $X = 1 \rightarrow heads$  
  - $P(X = x) = (\frac{1}{2})^x(\frac{1}{2})^{1-x}$ for $X = 0, 1$, where $x$ here represents a value we can plug into the PMF.  
  - A general form is $p(x) = (\theta)^x(1-\theta)^{1-x}$.  
* `dbinom(k, n, p)` = return the probability of getting `k` successes out of `n` trials, given probability of success is `p`.



### PDF (Probability Density Function)  
* The central dogma of __PDF__: Areas under PDFs correspond to probabilities for that random variable.  
* Two conditions to justify a mathematically valid density:  
  - Always $\ge0$  everywhere
  - Total area under the whole curve must $=1$
* **areas under PDFs** correspond to the probabilities for that random variable taking on that range of values (PMF)
* *Example:* when one says that intelligence quotients (IQ) in population follows a bell curve, they are saying that the *probability of a randomly selected person from this population having an IQ between two values is given by the area under the bell curve*.
* *Example:* suppose that the proportion of help calls that get addressed in a random day by a help line is given by $f(x) = 2x$ for $0 < x < 1$. What is the probability that 75% or fewer of calls get addressed?
* The probability is the area: ${1.5 \times 0.75 \over 2} = 0.5625$  

```{r}
x <- c(-0.5, 0, 1, 1, 1.5)
y <- c(0, 0, 2, 0, 0)
plot(x, y, lwd = 3,frame = FALSE, type = "l")
```
This special density is the beta density:  
```{r}
pbeta(0.75, 2, 1)
```


```{r echo = FALSE, fig.width = 2, fig.height = 2, fig.align = 'center'}
grid.raster(readPNG("figures/4-1.png"))
```


* but the probability of the variable taking a specific value = 0 (area of a line is 0)

```{r echo = FALSE, fig.width = 3, fig.height = 3, fig.align = 'center'}
grid.raster(readPNG("figures/4-2.png"))
```
* ***Note**: the above is true because it is modeling random variables as if they have infinite precision, when in reality they do not *

* `dnorm()`, `dgamma()`, `dpois()`, `dunif()` = return probability of a certain value from the normal, Gamma, Poisson, and uniform distributions

### Cumulative Distribution Function (CDF)
* CDF of a random variable $X$ = probability that the random variable is $\le$ value $x$
	* $F(x) = P(X \le x)$ = applies when $X$ is discrete/continuous
* PDF = derivative of CDF
	* integrate PDF $\rightarrow$ CDF
		* `integrate(function, lower=0, upper=1)` $\rightarrow$ can be used to evaluate integrals for a specified range
* `pbinom()`, `pnorm()`, `pgamma()`, `ppois()`, `punif()` = returns the cumulative probabilities from 0 up to a specified value from the binomial, normal, Gamma, Poisson, and uniform distributions


### Survival Function
* survival function of a random variable $X$ = probability the random variable $> x$, complement of CDF
    * $S(x) = P(X > x) = 1 - F(x)$, where $F(x) =$ CDF


### Quantile
* the $\alpha^{th}$ quantile of a distribution with distribution function F = point $x_{\alpha}$
	* $F(x_{\alpha}) = \alpha$
	* percentile = quantile with $\alpha$ expressed as a percent
	* median = 50^th^ percentile
	* $\alpha$% of the possible outcomes lie below it

```{r echo = FALSE, fig.width = 3, fig.height = 3, fig.align = 'center'}
grid.raster(readPNG("figures/5.png"))
```

* `qbeta(quantileInDecimals, 2, 1)` = returns quantiles for beta distribution
    * works for `qnorm()`, `qbinom()`, `qgamma()`, `qpois()`, etc.
* median estimated in this fashion = a population median
* probability model connects data to population using assumptions
	* population median = ***estimand***, sample median = ***estimator***

### Independence
* two events $A$ and $B$ are ***independent*** if the following is true
	* $P(A\:\cap\:B) = P(A)P(B)$
	* $P(A\:|\:B) = P(A)$
* two random variables $X$ and $Y$ are ***independent***, if for any two sets, **A** and **B**, the following is true
	* $P([X \in A]\cap[Y \in B]) = P(X \in A)P(Y \in B)$
* **independence** = statistically unrelated from one another
* if $A$ is ***independent*** of $B$, then the following are true
	* $A^c$ is independent of $B$
	* $A$ is independent of $B^c$
	* $A^c$ is independent of $B^c$


### IID Random Variables
* random variables are said to be **IID** if they are ***independent and identically distributed***
	* **independent** = statistically unrelated from each other
	* **identically distributed** = all having been drawn from the same population distribution
* IID random variables = default model for random samples = default starting point of inference



## Diagnostic Test
* Let $+$ and $-$ be the results, positive and negative respectively, of a diagnostic test
* Let $D$ = subject of the test has the disease, $D^c$ = subject does not
* **sensitivity** = $P(+\:|\:D)$ = probability that the test is positive given that the subject has the disease (the higher the better)
* **specificity** = $P(-\:|\:D^c)$ = probability that the test is negative given that the subject does not have the disease (the higher the better)
* **positive predictive value** = $P(D\:|\:+)$ = probability that that subject has the disease given that the test is positive
* **negative predictive value** = $P(D^c\:|\:-)$ = probability that the subject does not have the disease given the test is negative
* **prevalence of disease** = $P(D)$ = marginal probability of disease

### Example
* specificity of 98.5%, sensitivity = 99.7%, prevalence of disease = .1%
$$\begin{aligned}
P(D ~|~ +) & = \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}\\
& = \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + \{1-P(-~|~D^c)\}\{1 - P(D)\}} \\
& = \frac{.997\times .001}{.997 \times .001 + .015 \times .999}\\
& =  .062
\end{aligned}$$
* low positive predictive value $\rightarrow$ due to low prevalence of disease and somewhat modest specificity
	* suppose it was know that the subject uses drugs and has regular intercourse with an HIV infect partner (his probability of being + is higher than suspected)
	* evidence implied by a positive test result


### Likelihood Ratios
* **diagnostic likelihood ratio** of a **positive** test result is defined as $$DLR_+ = \frac{sensitivity}{1-specificity} =  \frac{P(+\:|\:D)}{P(+\:|\:D^c)}$$
* **diagnostic likelihood ratio** of a **negative** test result is defined as $$DLR_- = \frac{1 - sensitivity}{specificity} =  \frac{P(-\:|\:D)}{P(-\:|\:D^c)}$$
* from Baye's Rules, we can derive the *positive predictive value* and *false positive value*
$$P(D\:|\:+) = \frac{P(+\:|\:D)P(D)}{P(+\:|\:D)P(D)+P(+\:|\:D^c)P(D^c)}~~~~~~\mbox{(1)}$$
$$P(D^c\:|\:+) = \frac{P(+\:|\:D^c)P(D^c)}{P(+\:|\:D)P(D)+P(+\:|\:D^c)P(D^c)}~~~~~~\mbox{(2)}$$
* if we divide equation $(1)$ over $(2)$, the quantities over have the same denominator so we get the following $$\frac{P(D\:|\:+)}{P(D^c\:|\:+)} = \frac{P(+\:|\:D)}{P(+\:|\:D^c)} \times \frac{P(D)}{P(D^c)}$$ which can also be written as $$\mbox{post-test odds of D} = DLR_+ \times \mbox{pre-test odds of D}$$
	* **odds** = $p/(1-p)$
	* $\frac{P(D)}{P(D^c)}$ = **pre-test odds**, or odds of disease in absence of test
	* $\frac{P(D\:|\:+)}{P(+\:|\:D^c)}$ = **post-test odds**, or odds of disease given a positive test result
	* $DLR_+$ = factor by which the odds in the presence of a positive test can be multiplied to obtain the post-test odds
	* $DLR_-$ = relates the decrease in odds of disease after a negative result
* following the previous example, for sensitivity of 0.997 and specificity of 0.985, so the diagnostic likelihood ratios are as follows $$DLR_+ = .997/(1-.985) = 66 ~~~~~~ DLR_- =(1-.997)/.985 = 0.003$$
	* this indicates that the result of the positive test is the odds of disease is 66 times the pretest odds





## Expected Values
* __Expected Values__, including __Mean__, __Variance__, __Skewness__, etc. characterize a distribution. They are the properties of distributions.  
* __Mean__ characterizes of the center of a density or mass function.
 the distribution = *expected value*
* expected value operation = ***linear*** $\rightarrow$ $E(aX +bY) = aE(X) + bE(Y)$
* __Variance__ characterizes how spread out the distribution is.
* __Skewness__ considers how much a density is pulled toward high or low values.
* *Note:in this part we are discussing __Population__ quantities*
* _sample_ expected values for sample mean and variance will estimate the _population_ counterparts

* **population mean**
	* expected value/mean of a random variable = center of its distribution (center of mass)
	* ***discrete variables***
		* for $X$ with PMF $p(x)$, the population mean is defined as $$E[X] = \sum_{x} xp(x)$$ where the sum is taken over ***all*** possible values of $x$
		* $E[X]$ = center of mass of a collection of location and weights ${x,~p(x)}$
        * *coin flip example*: $E[X] = 0 \times (1-p) + 1 \times p = p$
	* ***continuous variable***
		* for $X$ with PDF $f(x)$, the expected value = the center of mass of the density
		* instead of summing over discrete values, the expectation ***integrates*** over a continuous function
			* PDF = $f(x)$
			* $\int xf(x)$ = area under the PDF curve = mean/expected value of $X$

* **sample mean**
	* sample mean estimates the population mean
		* sample mean = center of mass of observed data = empirical mean
        $$\bar X = \sum_{x}^n x_i p(x_i)$$ where $p(x_i) = 1/n$

```{r fig.width = 5, fig.height = 3, fig.align = 'center', message = F, warning = F}
# load relevant packages
library(UsingR); data(galton); library(ggplot2)
# plot galton data
g <- ggplot(galton, aes(x = child))
# add histogram for children data
g <- g + geom_histogram(fill = "salmon", binwidth=1, aes(y=..density..), colour="black")
# add density smooth
g <- g + geom_density(size = 2)
# add vertical line
g <- g + geom_vline(xintercept = mean(galton$child), size = 2)
# print graph
g
```


* **average of random variables** = a new random variable where its distribution has an expected value that is the **same** as the original distribution (centers are the same)
	* the mean of the averages = average of the original data $\rightarrow$ estimates average of the population
	* if $E[sample~mean]$ = population mean, then estimator for the sample mean is **unbiased**
		* [**derivation**] let $X_1$, $X_2$, $X_3$, ... $X_n$ be a collection of $n$ samples from the population with mean $\mu$
		* mean of this sample $$\bar X = \frac{X_1 + X_2 + X_3 +  .  + X_n}{n}$$
		* since $E(aX) = aE(X)$, the expected value of the mean is can be written as $$E\left[\frac{X_1 + X_2 + X_3 + ... + X_n}{n}\right] = \frac{1}{n} \times [E(X_1) + E(X_2) + E(X_3) + ... + E(X_n)]$$
		* since each of the $E(X_i)$ is drawn from the population with mean $\mu$, the expected value of each sample should be $$E(X_i) = \mu$$
		* therefore $$\begin{aligned}
E\left[\frac{X_1 + X_2 + X_3 + ... + X_n}{n}\right] & = \frac{1}{n} \times [E(X_1) + E(X_2) + E(X_3) + ... + E(X_n)]\\
& = \frac{1}{n} \times [\mu + \mu + \mu + ... + \mu]\\
& = \frac{1}{n} \times n \times \mu\\
& = \mu\\
\end{aligned}$$
* **Note**: the more data that goes into the sample mean, the more concentrated its density/mass functions are around the population mean *

```{r fig.width = 6, fig.height = 3, fig.align = 'center', message = F, warning = F}
nosim <- 1000
# simulate data for sample size 1 to 4
dat <- data.frame(
	x = c(sample(1 : 6, nosim, replace = TRUE),
        apply(matrix(sample(1 : 6, nosim * 2, replace = TRUE), nosim), 1, mean),
        apply(matrix(sample(1 : 6, nosim * 3, replace = TRUE), nosim), 1, mean),
        apply(matrix(sample(1 : 6, nosim * 4, replace = TRUE), nosim), 1, mean)),
	size = factor(rep(1 : 4, rep(nosim, 4))))
# plot histograms of means by sample size
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.25, colour = "black")
g + facet_grid(. ~ size)
```
