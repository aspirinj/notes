

### Confidence Intervals - Normal Distribution/Z Intervals
* **Z confidence interval** is defined as $$Estimate \pm ZQ \times SE_{Estimate}$$ where $ZQ$ = quantile from the standard normal distribution
* according to CLT, the sample mean, $\bar X$, is approximately normal with mean $\mu$ and sd $\sigma / \sqrt{n}$
* **95% confidence interval for the population mean $\mu$** is defined as $$\bar X \pm 2\sigma/\sqrt{n}$$ for the sample mean $\bar X \sim N(\mu, \sigma^2/n)$
	* you can choose to use 1.96 to be more accurate for the confidence interval
	* $P(\bar{X} > \mu + 2\sigma/\sqrt{n}~or~\bar{X} < \mu - 2\sigma/\sqrt{n}) = 5\%$
    * **interpretation**: if we were to repeatedly draw samples of size $n$ from the population and construct this confidence interval for each case, approximately 95% of the intervals will contain $\mu$
* confidence intervals get **narrower** with less variability or
larger sample sizes
* ***Note**: Poisson and binomial distributions have exact intervals that don't require CLT *
* ***example***
	- for this example, we will compute the 95% confidence interval for sons height data in inches

```{r message = F, warning = F}
# load son height data
data(father.son); x <- father.son$sheight
# calculate confidence interval for sons height in inches
mean(x) + c(-1, 1) * qnorm(0.975) * sd(x)/sqrt(length(x))
```


### Confidence Interval - Bernoulli Distribution/Wald Interval
* for Bernoulli distributions, $X_i$ is 0 or 1 with success probability $p$ and the variance is $\sigma^2 = p(1 - p)$
* the confidence interval takes the form of $$\hat{p} \pm z_{1-\alpha/2}\sqrt{\frac{p(1-p)}{n}}$$
* since the population proportion $p$ is unknown, we can use the sampled proportion of success $\hat{p} = X/n$ as estimate
* $p(1-p)$ is largest when $p = 1/2$, so 95% confidence interval can be calculated by $$\begin{aligned}
\hat{p} \pm Z_{0.95} \sqrt{\frac{0.5(1-0.5)}{n}} & = \hat{p} \pm qnorm(.975) \sqrt{\frac{1}{4n}}\\
& = \hat{p} \pm 1.96 \sqrt{\frac{1}{4n}}\\
& = \hat{p} \pm \frac{1.96}{2} \sqrt{\frac{1}{n}}\\
& \approx \hat{p} \pm \frac{1}{\sqrt{n}}\\
\end{aligned}$$
	* this is known as the **Wald Confidence Interval** and is useful in ***roughly estimating*** confidence intervals
	* generally need $n$ = 100 for 1 decimal place, 10,000 for 2, and 1,000,000 for 3
* ***example***
	- suppose a random sample of 100 likely voters, 56 intent to vote for you, can you secure a victory?
	- we can use the Wald interval to quickly estimate the 95% confidence interval
	- as we can see below, because the interval [`r 0.56 + c(-1, 1) * 1/sqrt(100)`] contains values below 50%, victory is not guaranteed
	- `binom.test(k, n)$conf` = returns confidence interval binomial distribution (collection of Bernoulli trial) with `k` successes in `n` draws

```{r message = F, warning = F}
# define sample probability and size
p = 0.56; n = 100
# Wald interval
c("WaldInterval" = p + c(-1, 1) * 1/sqrt(n))
# 95% confidence interval
c("95CI" = p + c(-1, 1) * qnorm(.975) * sqrt(p * (1-p)/n))
# perform binomial test
binom.test(p*100, n*100)$conf.int
```


### Confidence Interval - Binomial Distribution/Agresti-Coull Interval
* for a binomial distribution with smaller values of $n$ (when $n$ < 30, thus not large enough for CLT), often time the normal confidence intervals, as defined by $$\hat{p} \pm z_{1-\alpha/2}\sqrt{\frac{p(1-p)}{n}}$$ **do not** provide accurate estimates

```{r fig.width = 4, fig.height = 3, fig.align = 'center', message = F, warning = F}
# simulate 1000 samples of size 20 each
n <- 20; nosim <- 1000
# simulate for p values from 0.1 to 0.9
pvals <- seq(.1, .9, by = .05)
# calculate the confidence intervals
coverage <- sapply(pvals, function(p){
	# simulate binomial data
	phats <- rbinom(nosim, prob = p, size = n) / n
	# calculate lower 95% CI bound
	ll <- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)
	# calculate upper 95% CI bound
	ul <- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)
	# calculate percent of intervals that contain p
	mean(ll < p & ul > p)
})
# plot CI results vs 95%
ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(.75, 1.0)
```

* as we can see from above, the interval do not provide adequate coverage as 95% confidence intervals (frequently only provide 80 to 90% coverage)
* we can construct the **Agresti-Coull Interval**, which is defined uses the adjustment $$\hat{p} = \frac{X+2}{n+4}$$ where we effectively ***add 2*** to number of successes, $X$, and ***add 2*** to number of failure
* therefore the interval becomes $$\frac{X+2}{n+4} \pm z_{1-\alpha/2}\sqrt{\frac{p(1-p)}{n}}$$
* ***Note**: interval tend to be **conservative** *
* ***example***

```{r fig.width = 4, fig.height = 3, fig.align = 'center', message = F, warning = F}
# simulate 1000 samples of size 20 each
n <- 20; nosim <- 1000
# simulate for p values from 0.1 to 0.9
pvals <- seq(.1, .9, by = .05)
# calculate the confidence intervals
coverage <- sapply(pvals, function(p){
	# simulate binomial data with Agresti/Coull Interval adjustment
	phats <- (rbinom(nosim, prob = p, size = n) + 2) / (n + 4)
		# calculate lower 95% CI bound
	ll <- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)
	# calculate upper 95% CI bound
	ul <- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)
	# calculate percent of intervals that contain p
	mean(ll < p & ul > p)
})
# plot CI results vs 95%
ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(.75, 1.0)
````

* as we can see from above, the coverage is much better for the 95% interval
* in fact, all of the estimates are more conservative as we previously discussed, indicating the Agresti-Coull intervals are ***wider*** than the regular confidence intervals


### Confidence Interval - Poisson Interval
* for $X \sim Poisson(\lambda t)$
	* estimate rate $\hat{\lambda} = X/t$
	* $var(\hat{\lambda}) = \lambda/t$
	* variance estimate $= \hat{\lambda}/t$
* so the confidence interval is defined as $$\hat \lambda \pm z_{1-\alpha/2}\sqrt{\frac{\lambda}{t}}$$
	* however, for small values of $\lambda$ (few events larger time interval), we **should not** use the asymptotic interval estimated
	* ***example***
		- for this example, we will go through a specific scenario as well as a simulation exercise to demonstrate the ineffectiveness of asymptotic intervals for small values of $\lambda$
		- nuclear pump failed 5 times out of 94.32 days, give a 95% confidence interval for the failure rate per day?
		+ `poisson.test(x, T)$conf` = returns Poisson 95% confidence interval for given `x` occurrence over `T` time period

```{r fig.width = 4, fig.height = 3, fig.align = 'center', message = F, warning = F}
# define parameters
x <- 5; t <- 94.32; lambda <- x / t
# calculate confidence interval
round(lambda + c(-1, 1) * qnorm(.975) * sqrt(lambda / t), 3)
# return accurate confidence interval from poisson.test
poisson.test(x, T = 94.32)$conf
# small lambda simulations
lambdavals <- seq(0.005, 0.10, by = .01); nosim <- 1000; t <- 100
# calculate coverage using Poisson intervals
coverage <- sapply(lambdavals, function(lambda){
	# calculate Poisson rates
	lhats <- rpois(nosim, lambda = lambda * t) / t
	# lower bound of 95% CI
	ll <- lhats - qnorm(.975) * sqrt(lhats / t)
	# upper bound of 95% CI
	ul <- lhats + qnorm(.975) * sqrt(lhats / t)
	# calculate percent of intervals that contain lambda
	mean(ll < lambda & ul > lambda)
})
# plot CI results vs 95%
ggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ylim(0, 1.0)
```

* as we can see above, for small values of $\lambda = X/t$, the confidence interval produced by the asymptotic interval is ***not*** an accurate estimate of the actual 95% interval (not enough coverage)
* however, as $t \to \infty$, the interval becomes the ***true 95% interval***


```{r fig.width = 4, fig.height = 3, fig.align = 'center', message = F, warning = F}
# small lambda simulations
lambdavals <- seq(0.005, 0.10, by = .01); nosim <- 1000; t <- 1000
# calculate coverage using Poisson intervals
coverage <- sapply(lambdavals, function(lambda){
	# calculate Poisson rates
	lhats <- rpois(nosim, lambda = lambda * t) / t
	# lower bound of 95% CI
	ll <- lhats - qnorm(.975) * sqrt(lhats / t)
	# upper bound of 95% CI
	ul <- lhats + qnorm(.975) * sqrt(lhats / t)
	# calculate percent of intervals that contain lambda
	mean(ll < lambda & ul > lambda)
})
# plot CI results vs 95%
ggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(0, 1.0)
```

* as we can see from above, as $t$ increases, the Poisson intervals become closer to the actual 95% confidence intervals



### Confidence Intervals - T Distribution(Small Samples)
* **t** confidence interval is defined as $$Estimate \pm TQ \times SE_{Estimate} = \bar X \pm \frac{t_{n-1} S}{\sqrt{n}}$$
	* $TQ$ = quantile from T distribution
    * $t_{n-1}$ = relevant quantile
    * $t$ interval assumes data is IID normal so that $$\frac{\bar X - \mu}{S/\sqrt{n}}$$ follows Gosset's $t$ distribution with $n-1$ degrees of freedom
    * works well with data distributions that are roughly symmetric/mound shaped, and ***does not*** work with skewed distributions
        * skewed distribution $\rightarrow$ meaningless to center interval around the mean $\bar X$
        * logs/median can be used instead
    * paired observations (multiple measurements from same subjects) can be analyzed by t interval of differences
    * as more data collected (large degrees of freedom), t interval $\rightarrow$ z interval
    * `qt(0.975, df=n-1)` = calculate the relevant quantile using t distribution

```{r fig.width = 6, fig.height = 3, fig.align = 'center', message = F, warning = F}
# Plot normal vs t distributions
k <- 1000; xvals <- seq(-5, 5, length = k); df <- 10
d <- data.frame(y = c(dnorm(xvals), dt(xvals, df)),x = xvals,
              dist = factor(rep(c("Normal", "T"), c(k,k))))
g <- ggplot(d, aes(x = x, y = y))
g <- g + geom_line(size = 2, aes(colour = dist)) + ggtitle("Normal vs T Distribution")
# plot normal vs t quantiles
d <- data.frame(n= qnorm(pvals),t=qt(pvals, df),p = pvals)
h <- ggplot(d, aes(x= n, y = t))
h <- h + geom_abline(size = 2, col = "lightblue")
h <- h + geom_line(size = 2, col = "black")
h <- h + geom_vline(xintercept = qnorm(0.975))
h <- h + geom_hline(yintercept = qt(0.975, df)) + ggtitle("Normal vs T Quantiles")
# plot 2 graphs together
grid.arrange(g, h, ncol = 2)
```


* William Gosset's **t** Distribution ("Student's T distribution")
	* test = Gosset's pseudoname which he published under
	* indexed/defined by ***degrees of freedom***, and becomes more like standard normal as degrees of freedom gets larger
	* thicker tails centered around 0, thus confidence interval = ***wider*** than Z interval (more mass concentrated away from the center)
	* for ***small*** sample size (value of n), normalizing the distribution by $\frac{\bar X - \mu}{S/\sqrt{n}}$ $\rightarrow$ t distribution, ***not*** the standard normal distribution
		* $S$ = standard deviation may be inaccurate, as the std of the data sample may not be truly representative of the population std
        * using the Z interval here thus may produce an interval that is too ***narrow***

### Confidence Interval - Paired T Tests
* compare observations for the same subjects over two different sets of data (i.e. different times, different treatments)
* the confidence interval is defined by $$ \bar X_1 - \bar X_2 \pm \frac{t_{n-1} S}{\sqrt{n}}$$ where $\bar X_1$ represents the first observations and $\bar X_2$ the second set of observations
* `t.test(difference)` = performs group mean t test and returns metrics as results, which includes the confidence intervals
	- `t.test(g2, g1, paired = TRUE)` = performs the same paired t test with data directly
* ***example***
	- the data used here is for a study of the effects of two soporific drugs (increase in hours of sleep compared to control) on 10 patients

```{r fig.width = 4, fig.height = 3, fig.align = 'center', message = F, warning = F}
# load data
data(sleep)
# plot the first and second observations
g <- ggplot(sleep, aes(x = group, y = extra, group = factor(ID)))
g <- g + geom_line(size = 1, aes(colour = ID)) + geom_point(size =10, pch = 21, fill = "salmon", alpha = .5)
g
# define groups
g1 <- sleep$extra[1 : 10]; g2 <- sleep$extra[11 : 20]
# define difference
difference <- g2 - g1
# calculate mean and sd of differences
mn <- mean(difference); s <- sd(difference); n <- 10
# calculate intervals manually
mn + c(-1, 1) * qt(.975, n-1) * s / sqrt(n)
# perform the same test to get confidence intervals
t.test(difference)
t.test(g2, g1, paired = TRUE)
```


### Independent Group **t** Intervals - Same Variance
* compare two groups in randomized trial ("A/B Testing")
* cannot use the paired t test because the groups are independent and may have different sample sizes
* perform randomization to balance unobserved covariance that may otherwise affect the result
* $t$ confidence interval for $\mu_y - \mu_x$ is defined as $$\bar Y - \bar X \pm t_{n_x + n_y - 2, 1 - \alpha/2}S_p\left(\frac{1}{n_x} + \frac{1}{n_y}\right)^{1/2}$$
    * $t_{n_x + n_y - 2, 1 - \alpha/2}$ = relevant quantile
    * $n_x + n_y - 2$ = degrees of freedom
    * $S_p\left(\frac{1}{n_x} + \frac{1}{n_y}\right)^{1/2}$ = standard error
    * $S_p^2 = \{(n_x - 1) S_x^2 + (n_y - 1) S_y^2\}/(n_x + n_y - 2)$ = pooled variance estimator
        * this is effectively a weighted average between the two variances, such that different sample sizes are taken in to account
        * For equal sample sizes, $n_x = n_y$, $S_p^2 = \frac{S_x^2 + S_y^2}{2}$ (average of variance of two groups)
    * ***Note:** this interval assumes **constant variance** across two groups; if variance is different, use the next interval *

### Independent Group t Intervals - Different Variance
* confidence interval for $\mu_y - \mu_x$ is defined as $$\bar Y - \bar X \pm t_{df} \times \left(\frac{s_x^2}{n_x} + \frac{s_y^2}{n_y}\right)^{1/2}$$
    * $t_{df}$ = relevant quantile with df as defined below
    * ***Note**: normalized statistic does not follow t distribution but can be approximated through the formula with df defined below *
        $$df = \frac{\left(S_x^2 / n_x + S_y^2/n_y\right)^2}
        {\left(\frac{S_x^2}{n_x}\right)^2 / (n_x - 1) +
        \left(\frac{S_y^2}{n_y}\right)^2 / (n_y - 1)}$$
        * $\left(\frac{s_x^2}{n_x} + \frac{s_y^2}{n_y}\right)^{1/2}$ = standard error

* Comparing other kinds of data
    * binomial $\rightarrow$ relative risk, risk difference, odds ratio
    * binomial $\rightarrow$ Chi-squared test, normal approximations, exact tests
    * count $\rightarrow$ Chi-squared test, exact tests

* R commands
    * t Confidence Intervals
        * `mean + c(-1, 1) * qt(0.975, n - 1) * std / sqrt(n)`
            * ***c(-1, 1)*** = plus and minus, $\pm$

    * Difference Intervals (all equivalent)
        * `mean2 - mean1 + c(-1, 1) * qt(0.975, n - 1) * std / sqrt(n)`
            * ***n*** = number of paired observations
            * ***qt(0.975, n - 1)*** =  relevant quantile for paired
            * ***qt(0.975, n$_x$ + n$_y$ - 2)*** =  relevant quantile for independent
        * `t.test(mean2 - mean1)`
        * `t.test(data2, data1, paired = TRUE, var.equal = TRUE)`
            * ***paired*** = whether or not the two sets of data are paired (same subjects different observations for treatment) $\rightarrow$ `TRUE` for paired, `FALSE` for independent
            * ***var.equal*** = whether or not the variance of the datasets should be treated as equal $\rightarrow$ `TRUE` for same variance, `FALSE` for unequal variances
        * `t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep)`
            * ***relevel(factor, ref)*** = reorders the levels in the factor so that "ref" is changed to the first level $\rightarrow$ doing this here is so that the second set of measurements come first (1, 2 $\rightarrow$ 2, 1) in order to perform mean$_2$ - mean$_1$
            * ***I(object)*** = prepend the class "AsIs" to the object
            * ***Note**: I(relevel(group, 2)) = explanatory variable, must be **factor** and have **two levels** *



## Hypothesis Testing
* Hypothesis testing = making decisions using data
    * **null** hypothesis (**H$_0$**) = status quo
    * assumed to be ***true*** $\rightarrow$ statistical evidence required to reject it for **alternative** or "research" hypothesis (**H$_a$**)
        * alternative hypothesis typically take form of >, < or $\ne$
    * **Results**

Truth | Decide | Result |
---|---|---|
$H_0$ | $H_0$ | Correctly accept null |
$H_0$ | $H_a$ | Type I error |
$H_a$ | $H_a$ | Correctly reject null |
$H_a$ | $H_0$ | Type II error |

* **$\alpha$** = Type I error rate
    * probability of ***rejecting*** the null hypothesis when the hypothesis is ***correct***
    * $\alpha$ = 0.05 $\rightarrow$ standard for hypothesis testing
    * ***Note**: as Type I error rate increases, Type II error rate decreases and vice versa *

* for large samples (large n), use the **Z Test** for $H_0:\mu = \mu_0$
    * **$H_a$:**
        * $H_1: \mu < \mu_0$
        * $H_2: \mu \neq \mu_0$
        * $H_3: \mu > \mu_0$
    * Test statistic $TS = \frac{\bar{X} - \mu_0}{S / \sqrt{n}}$
    * Reject the null hypothesis $H_0$ when
        * $H_1: TS \leq Z_{\alpha}$ OR $-Z_{1 - \alpha}$
        * $H_2: |TS| \geq Z_{1 - \alpha / 2}$
        * $H_3: TS \geq Z_{1 - \alpha}$
    * ***Note**: In case of $\alpha$ = 0.05 (most common), $Z_{1-\alpha}$ = 1.645 (95 percentile) *
    * $\alpha$ = low, so that when $H_0$ is rejected, original model $\rightarrow$ wrong or made an error (low probability)

* For small samples (small n), use the **T Test** for $H_0:\mu = \mu_0$
    * **$H_a$:**
        * $H_1: \mu < \mu_0$
        * $H_2: \mu \neq \mu_0$
        * $H_3: \mu > \mu_0$
    * Test statistic $TS = \frac{\bar{X} - \mu_0}{S / \sqrt{n}}$
    * Reject the null hypothesis $H_0$ when
        * $H_1: TS \leq T_{\alpha}$ OR $-T_{1 - \alpha}$
        * $H_2: |TS| \geq T_{1 - \alpha / 2}$
        * $H_3: TS \geq T_{1 - \alpha}$
    * ***Note**: In case of $\alpha$ = 0.05 (most common), $T_{1-\alpha}$ = `qt(.95, df = n-1)` *
    * R commands for T test:
        * `t.test(vector1 - vector2)`
        * `t.test(vector1, vector2, paired = TRUE)`
            * `alternative` argument can be used to specify one-sided tests: `less` or `greater`
            * `alternative` default = `two-sided`
        * prints test statistic (`t`), degrees of freedom (`df`), `p-value`, 95% confidence interval, and mean of sample
            * confidence interval in units of data, and can be used to intepret the practical significance of the results

* **rejection region** = region of TS values for which you reject $H_0$
* **power** = probability of rejecting $H_0$
    * power is used to calculate sample size for experiments

* **two-sided tests** $\rightarrow$ $H_a: \mu \neq \mu_0$
    * reject $H_0$ only if test statistic is too larger/small
    * for $\alpha$ = 0.05, split equally to 2.5% for upper and 2.5% for lower tails
        * equivalent to $|TS| \geq T_{1 - \alpha / 2}$
        * example: for T test, `qt(.975, df)` and `qt(.025, df)`
    * ***Note**: failing to reject one-sided test = fail to reject two-sided*

* **tests vs confidence intervals**
    * ($1-\alpha$)% confidence interval for $\mu$ = set of all possible values that fail to reject $H_0$
    * if ($1-\alpha$)% confidence interval contains $\mu_0$, fail to reject $H_0$

* **two-group intervals/test**
    * Rejection rules the same
    * Test $H_0$: $\mu_1 = \mu_2$ $\rightarrow$ $\mu_1 - \mu_2 = 0$
    * Test statistic:
    $$\frac{Estimate - H_0 Value}{SE_{Estimate}} = \frac{\bar X_1 - \bar X_2 - 0}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}}$$
    * R Command
        * `t.test(values ~ factor, paired = FALSE, var.equal = TRUE, data = data)`
            * `paired = FALSE` = independent values
            * `factor` argument must have only two levels

* **p values**
    * most common measure of statistical significance
    * **p-value** = probability under the null hypothesis of obtaining evidence as extreme or more than that of the obtained
        * Given that $H_0$ is true, how likely is it to obtain the result (test statistic)?
    * **attained significance level** = smallest value for $\alpha$ for which $H_0$ is rejected $\rightarrow$ equivalent to p-value
        * if p-value < $\alpha$, reject $H_0$
        * for two-sided tests, double the p-values
    * if p-value is small, either $H_0$ is true AND the obeserved is a rare event **OR** $H_0$ is false
    * R Command
        * p-value = `pt(statistic, df, lower.tail = FALSE)`
            * `lower.tail = FALSE` = returns the probability of getting a value from the t distribution that is larger than the test statistic
        * Binomial (coin flips)
            * probability of getting x results out of n trials and event probability of p = `pbinom(x, size = n, prob = p, lower.tail = FALSE)`
            * two-sided interval (testing for $\ne$): find the smaller of two one-sided intervals (X < value, X > value), and double the result
            * ***Note**: `lower.tail = FALSE` = strictly greater *
        * Poisson
            * probability of getting x results given the rate r = `ppois(x - 1, r, lower.tail = FALSE)`
            * `x - 1` is used here because the upper tail includes the specified number (since we want greater than x, we start at x - 1)
            * `r` = events that should occur given the rate (multiplied by 100 to yield an integer)
            * **Note**: `lower.tail = FALSE` = strictly greater
